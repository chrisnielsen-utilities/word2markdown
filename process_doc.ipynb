{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b9849a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613bfa98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b535d7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "document = Document('Knowledge_test.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d0b5d9c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Document' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14048/2371361720.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Document' object is not iterable"
     ]
    }
   ],
   "source": [
    "for x in document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7920da80",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "index() missing 1 required positional argument: 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14048/3867085023.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: index() missing 1 required positional argument: 'value'"
     ]
    }
   ],
   "source": [
    "for x in document.sections.index():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "63a850b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhD Knowledge Resources\n",
      "Information about different knowledge sources (e.g. podcasts and courses) that are useful for my PhD research\n",
      "Useful General Knowledge\n",
      "General knowledge about different things that is useful\n",
      " Lessons\n",
      "Lessons that I have learned through life.\n",
      "Pick an x value that we are evaluating\n",
      "Evaluate g(x) for that x value (this will return the y value that minimizes f(x,y) for that choice of x)\n",
      "Repeat step 1 until we find the value of x which maximizes g(x)\n",
      "Lesson date\n",
      "October 5, 2021\n",
      "Context\n",
      "Today I had a lab retreat at the university. I did not pack a lunch as I thought that there might be food provided. There was not. Therefore, I was forced to go and eat a pita from Extreme Pita. The pita was very salty and made me feel crappy. \n",
      "Lesson learned\n",
      "In the future I would plan to bring a lunch. If I forgot my lunch, then I would purchase a protein bar or in worse case a muffin. The salt content in the pita was simply too much.\n",
      "The gradient of a function will always be orthogonal to the level curve of that function at any given point. To see why, take the first order Taylor expansion around a given point. For perturbations around the point to remain on the level curve (i.e. g(x) = g(x+dx)), the perturbation direction must be orthogonal to the gradient. Therefore, the gradient of g(x) will always be orthogonal to the tangential direction of the curve g(x) = c.\n",
      "If f(x*) is a constrained minimum satisfying the constraint g(x*) = c, then the gradient of f(x*) is orthogonal to the level curve g(x*) = c. To see why, consider the following argument: if the gradient of f(x*) were not orthogonal to the level curve g(x*) = c, then it would be possible to move in a direction along the level curve g(x*) = c which would produce a better minimum which contradicts f(x*) being the constrained minimum (to be a constrained minimum, the directional derivative of f(x*) along the directions tangent to g(x*) = c must be nonnegative). \n",
      "The optimal solution \\[{x^*}\\]is inside the constraint region such that\\[g\\left( {{x^*}} \\right) < 0\\]. In this case, this constraint can be considered inactive. The solution can be found by simply solving \\[\\nabla f\\left( {{x^*}} \\right) = 0\\]. In this region, the dual variable can be set to 0, such that\\[\\alpha  = 0\\]. \n",
      "The optimal solution\\[{x^*}\\] lies on the constraint boundary such that \\[g\\left( {{x^*}} \\right) = 0\\]. In this case, we can apply the standard Laplacian stationarity constraint such that\\[\\nabla f\\left( {{x^*}} \\right) - \\alpha \\nabla g\\left( {{x^*}} \\right) = 0\\]. However, we can go further than this since we know that \\[\\alpha  > 0\\]. To see why, suppose that \\[\\nabla f\\left( {{x^*}} \\right)\\]was in the same direction as \\[\\nabla g\\left( {{x^*}} \\right)\\], then it is possible to move in a direction that simultaneously reduces g while reducing f. Hence, the constrained minimum would exist inside the constraint region and not on the boundary. Therefore, it follows that \\[\\nabla f\\left( {{x^*}} \\right)\\]must be in the opposite direction as \\[\\nabla g\\left( {{x^*}} \\right)\\]and \\[\\alpha  > 0\\].\n",
      "Primal feasibility: \\[g\\left( {{x^*}} \\right) \\le 0\\]\n",
      "Dual feasibility: \\[\\alpha  \\ge 0\\]\n",
      "Complementary slackness: \\[\\alpha g\\left( {{x^*}} \\right) = 0\\]\n",
      "Lagrangian stationarity: \n",
      "We start with some initial parameter estimates (theta_A, theta_B)\n",
      "We determine the most likely coin identity for each coin flip given the data and our parameter estimates\n",
      "Then we assume that these estimated coin flip identities are correct, and we apply maximum likelihood to get a new estimate for the parameters (theta_A, theta_B)\n",
      "We iterate steps 2 and 3 until convergence\n",
      "The EM algorithm is a great candidate for parameter estimation problems where solving the original maximum likelihood problem is hard or intractable, but the problem becomes much easier to solve if we could introduce some latent variables\n",
      "We can view the EM algorithm as alternating between: 1) estimating the conditional distribution for the latent variables using our current estimate of the parameters, 2) using this conditional distribution to re-estimate the parameter values.\n",
      "By using Jensen’s inequality over the log likelihood, we can come up with a lower bound for the likelihood function called the evidence lower bound (ELBO). The log likelihood will always >= than the ELBO. Therefore, by optimizing the ELBO we optimize the lower bound on the log likelihood.\n",
      "If we pick the distribution over the latent variables to equal the conditional probability of the latent variables given the parameters, then the ELBO will equal the log likelihood for that specific parameter value. Then, by optimizing the ELBO for the parameters, we can find a parameter choice which increases the lower bound for the log likelihood. This process is shown in the figure below. \n",
      "To justify that the EM algorithm converges we can show that the EM algorithm monotonically improves the log-likelihood each iteration.\n",
      "In summary, the ELBO is always a lower bound on the likelihood function. Therefore, for a given parameter choice, we can pick the latent distribution to maximize the ELBO for the current parameters, and then keep the distribution fixed and optimize the parameters over the ELBO. \n",
      "Start with the likelihood function that we want to maximize\\[p\\left( {x;\\theta } \\right)\\]for a set of parameters theta and observed data x \n",
      "Write this likelihood function as a marginalization over the latent variables\\[p\\left( {x;\\theta } \\right) = \\sum\\limits_z {p\\left( {x,z;\\theta } \\right)} \\]\n",
      "Write the log likelihood by taking the log of both sides \\[L\\left( \\theta  \\right) = \\log p\\left( {x;\\theta } \\right) = \\log \\sum\\limits_z {p\\left( {x,z;\\theta } \\right)} \\]\n",
      "Use Jensen’s inequality to write the log likelihood as an upper bound of the ELBO. Notice that the distribution Q is considered to be a fixed parameter \\[\\log p\\left( {x;\\theta } \\right) \\ge \\sum\\limits_z {Q\\left( z \\right)\\log \\frac{{p\\left( {x,z;\\theta } \\right)}}{{Q\\left( z \\right)}}}  = ELBO\\left( {x;Q,\\theta } \\right)\\]\n",
      "We notice that the inequality described above becomes an equality if we pick \\[Q\\left( z \\right) = p\\left( {z|x;\\theta } \\right)\\]for a specific setting of the parameters\\[\\theta \\]. \n",
      "Given Jensen’s inequality from step 4, we know that the ELBO is a lower bound on the log likelihood for any values of Q, theta, and x. Therefore, we can construct an iterative algorithm that works as follows: 1) start with some initial values for theta, 2) determine the conditional distribution Q such that the ELBO has equality with the log likelihood using the current value of theta, 3) optimize the ELBO to find a theta that increases the ELBO value and hence increases the lower bound of the log likelihood, 4) iterate until convergence\n",
      "We can justify that the EM algorithm converges by noting that the EM algorithm monotonically improves the log likelihood at each iteration.\n",
      "Another valuable interpretation of the ELBO is by writing it as follows:  \\[ELBO\\left( {x;Q,\\theta } \\right) = \\log p\\left( {x;\\theta } \\right) - {D_{KL}}\\left( {Q\\left( z \\right)||p\\left( {z|x;\\theta } \\right)} \\right)\\]. From this form, we clearly see that the ELBO is maximized when the KL divergence is 0 which happens when Q is picked to be the posterior distribution for z.  \n",
      "What is the probability of a given sequence of observations assuming we know the state transition matrix A and the observation generating matrix B\n",
      "What is the most likely series of hidden states that would generate a given sequence of observations assuming we know the state transition matrix A and the observation generating matrix B\n",
      "How can we learn the HMM parameters A and B given some observation data\n",
      "\n",
      "\n",
      "The Q projection of a word represents what kind of information the model would like to have (i.e. query), that would help inform how that word should be used. For example, in the above example sentence, we have the words “not” and “terrible” which act on each other to form a double negative. Therefore, it makes sense that the query embedding for the word “not” would inform the model that the information described in the word “terrible” would be useful for its interpretation.\n",
      "The K projection of a word represents what kind of information that word contains in its V embedding that could be used by the model. For example, the similarity between the Q embedding for the word “not” and the K embedding for the word “terrible”, describes how important it is for the model to utilize the information encoded in the V embedding for the word “terrible” to understand the concept of the word “not” in the context of the sentence. \n",
      "The V projection of a word represents what kind of information that model would find useful about that word to inform future processing. For example, the V embedding of the word “terrible” describes the kind of information that the model would find useful about this word.\n",
      "Encoder\n",
      "The input words are converted into word embeddings\n",
      "The word embeddings are combined with positional encoding information to inform the network which order in the sequence each word is located\n",
      "There are N layers of multi-head self-attention followed by a feed forward network. Each layer is normalized and also has a skip connection\n",
      "The output of the last encoder layer is used as the context vector for the decoder\n",
      "Decoder\n",
      "Similar to the encoder, the output words are converted into word embeddings and positionally encoded\n",
      "There are N layers of: 1) masked multi-head self-attention, 2) encoder-decoder attention, and 3) feed forward network\n",
      "The masked multi-head self-attention works similar to the multi-head self-attention from the encoder, with the exception that data that has not yet been generated is masked out\n",
      "The encoder-decoder attention mechanism is used to perform attention using information from the output of the encoder. Specifically, the output of the encoder is considered to be the keys and values, and the decoder input of the encoder-decoder attention layer is considered to be the query.\n",
      "The output of the final decoder layer is passed through a linear layer and then a final softmax to output the probability distribution over output words\n",
      "CS 480 – 2019 Lecture 13\n",
      "Zoya SVM Tutorial\n",
      "A Gaussian process can be interpreted as an infinite dimensional Gaussian random variable with mean function m(X) and covariance function k(X,X’)\n",
      "The covariance function can be any function that when produces positive semi-definite covariance matricies for the y_i’s with any allowed set of x_i’s. See  \n",
      "To sample a finite number of points from a GP, we can use the fact that a finite dimensional subset of the GP results in a marginal distribution that is Gaussian with mean vector m(X) and covariance matrix k(X,X) for a given finite sample X. \n",
      "Sampling is enabled by the fact that Gaussians have a really easy way to compute marginal distributions. Essentially, to compute the marginal distribution, you only keep the covariance and mean components for the values you are marginalizing on. This enables an infinite dimension GP to be marginalized to a finite dimension Gaussian distribution when sampling.\n",
      "The conditional distribution for a Gaussian is also straightforward to compute which enables the conditioning of the GP on specific data points and can produce a conditional distribution for regression.\n",
      "See the document “Gaussian Processes for Machine Learning” for more details\n",
      "Following slide 9 in “Gaussian Processes for Machine Learning”, write a script to sample from a GP using the squared exponential as the covariance function\n",
      "Code for this exercise is located in the python notebook GP_Exercises\n",
      "Following the section on Predictions from posterior here: , write a script to compute the conditional distribution for a GP, using the squared exponential as the covariance function and using points from the sine function as a prior.\n",
      "Code for this exercise is located in the python notebook GP_Exercises\n",
      "\n",
      "A Gaussian process is equivalent to a neural network with one hidden layer that is infinitely large.\n",
      "The core idea with a Gaussian Process is to have a distribution over functions\n",
      "If we assume that we have data that has no noise and we are trying to fit a function through this data.\n",
      "A gaussian process learns the Gaussian distribution of the output of a function at any point\n",
      "The gaussians for each point must be correlated with each other with correlation stronger when the distance between points is small\n",
      "A Gaussian process is an infinite dimensional Gaussian distribution\n",
      "The mean for the GP is a function (instead of a vector) and the covariance is a function (instead of a matrix)\n",
      "Website\n",
      "\n",
      "Notes\n",
      "We can interpret a GP as being an infinite dimensional Gaussian random variable. Therefore to sample points from the GP, ..\n"
     ]
    }
   ],
   "source": [
    "for paragraph in document.paragraphs:\n",
    "    if paragraph.style.name == 'List Paragraph':\n",
    "#         print(paragraph.style.name)\n",
    "        print(paragraph.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce388c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b478b327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Title'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph.style.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01c39d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29265cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx2txt\n",
    "\n",
    "# # extract text\n",
    "# text = docx2txt.process(\"file.docx\")\n",
    "\n",
    "# extract text and write images in /tmp/img_dir\n",
    "text = docx2txt.process(\"Knowledge_test.docx\", \"data\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e031d2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57768667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Knowledge\\n\\n\\n\\nThis document contains the following:\\n\\nPhD Knowledge Resources\\n\\nInformation about different knowledge sources (e.g. podcasts and courses) that are useful for my PhD research\\n\\nUseful General Knowledge\\n\\nGeneral knowledge about different things that is useful\\n\\n Lessons\\n\\nLessons that I have learned through life.\\n\\n\\n\\n\\n\\nPhD Knowledge Resources\\n\\n\\n\\nThis section contains Information about different knowledge sources (e.g. podcasts and courses) that are useful for my PhD research\\n\\n\\n\\n\\n\\nYouTube Channels\\n\\nTwo Minute Papers\\n\\nLex Fridman\\n\\nHenry AI Labs\\n\\nArXiv Insights\\n\\nYannic Kilcher\\n\\nLeo Isikdogan\\n\\nPreserve Knowledge\\n\\nKaggle Reading Group\\n\\n3Blue1Brown \\n\\nAmii Intelligence\\n\\nComputerphile \\n\\n\\n\\n\\n\\n\\n\\nBlogs\\n\\nhttps://www.reddit.com/r/MachineLearning/comments/mwwftu/d_your_favorite_ai_podcasts_blogs_newsletters/\\n\\nhttps://distill.pub/\\n\\nhttps://openai.com/blog/\\n\\nhttp://www.wildml.com/\\n\\nhttps://thegradient.pub/\\n\\n\\n\\nhttps://lilianweng.github.io/lil-log/archive.html\\n\\n\\n\\nCourses\\n\\nhttps://deep-learning-drizzle.github.io/\\n\\nhttps://www.reddit.com/r/MachineLearning/comments/fdw0ax/d_advanced_courses_update/\\n\\nhttps://github.com/luspr/awesome-ml-courses\\n\\nhttps://projects.iq.harvard.edu/stat110/home\\n\\nhttps://www.reddit.com/r/statistics/comments/982sdw/are_there_any_free_online_courses_for_advanced/\\n\\n\\n\\nhttps://cs229.stanford.edu/syllabus.html\\n\\n\\n\\nBooks\\n\\n\\n\\nPractical Statistics for Data Scientists: 50+ Essential Concepts Using R and Python\\n\\nFoundations of Statistics for Data Scientists: With R and Python\\n\\nThink Bayes: Bayesian Statistics in Python\\n\\nBayes' Rule With Python: A Tutorial Introduction to Bayesian Analysis\\n\\n\\n\\n\\n\\n\\n\\nCode\\n\\nhttps://paperswithcode.com/\\n\\n\\n\\n\\n\\n\\n\\nConferences\\n\\nNeurIPS\\n\\nCVPR\\n\\nECCV\\n\\nBMVC\\n\\nICCV\\n\\nMICCAI\\n\\nICML\\n\\nICLR\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOpen Questions\\n\\n\\n\\nThis section contains discussion around open questions I have\\n\\n\\n\\n\\n\\n\\n\\nInterpretation of min/max operations\\n\\n\\n\\nA question I have struggled with that is how to interpret the expression:\\n\\n\\\\[\\\\mathop {\\\\max }\\\\limits_x \\\\mathop {\\\\min }\\\\limits_y f\\\\left( {x,y} \\\\right)\\\\]\\n\\nSo far, I think the best way to interpret this expression is the following: consider breaking the expression into two components as follows:\\n\\n\\\\[\\\\mathop {\\\\max }\\\\limits_x \\\\mathop {\\\\min }\\\\limits_y f\\\\left( {x,y} \\\\right) = \\\\mathop {\\\\max }\\\\limits_x g\\\\left( x \\\\right)\\\\] where \\\\[g\\\\left( x \\\\right) = \\\\mathop {\\\\min }\\\\limits_y f\\\\left( {x,y} \\\\right)\\\\]\\n\\nTherefore, we can interpret this expression as:\\n\\nPick an x value that we are evaluating\\n\\nEvaluate g(x) for that x value (this will return the y value that minimizes f(x,y) for that choice of x)\\n\\nRepeat step 1 until we find the value of x which maximizes g(x)\\n\\n\\n\\nAnother way to interpret the min/max operations is to think about finding the saddle point of a function. Finding the saddle point is equivalent to finding the min/max solution for a set of variables.\\n\\n\\n\\n\\n\\n\\n\\nHere is a proof that for any function\\\\[f\\\\left( {x,y} \\\\right)\\\\]we have \\\\[\\\\mathop {\\\\max }\\\\limits_y \\\\mathop {\\\\min }\\\\limits_x f\\\\left( {x,y} \\\\right) \\\\le \\\\mathop {\\\\min }\\\\limits_x \\\\mathop {\\\\max }\\\\limits_y f\\\\left( {x,y} \\\\right)\\\\] \\n\\n\\n\\nProof\\n\\nSuppose that \\\\[\\\\left( {{x^*},{y^*}} \\\\right)\\\\]is the optimal solution to \\\\[\\\\mathop {\\\\min }\\\\limits_x \\\\mathop {\\\\max }\\\\limits_y f\\\\left( {x,y} \\\\right)\\\\], such that \\\\[\\\\mathop {\\\\min }\\\\limits_x \\\\mathop {\\\\max }\\\\limits_y f\\\\left( {x,y} \\\\right) = f\\\\left( {{x^*},{y^*}} \\\\right)\\\\]. This implies that \\\\[\\\\mathop {\\\\min }\\\\limits_x \\\\mathop {\\\\max }\\\\limits_y f\\\\left( {x,y} \\\\right) = \\\\mathop {\\\\max }\\\\limits_y f\\\\left( {{x^*},y} \\\\right)\\\\]since we can view the inner max operation as being evaluated for a specific x value, and since \\\\[{x^*}\\\\]is optimal, we can remove the min operation. \\n\\nNow let use examine the situation where the min and max operations are switched such that \\\\[\\\\mathop {\\\\max }\\\\limits_y \\\\mathop {\\\\min }\\\\limits_x f\\\\left( {x,y} \\\\right)\\\\]. Focusing on the inner min operation: since the inner min operation is the minimum over all x values then \\\\[\\\\mathop {\\\\min }\\\\limits_x f\\\\left( {x,y} \\\\right) \\\\le f\\\\left( {{x^*},y} \\\\right)\\\\].  Since this inequality must hold for all values of y, the inequality is preserved when we take the max operation of y on both sides of the inequality. Therefore, we have that \\\\[\\\\mathop {\\\\max }\\\\limits_y \\\\mathop {\\\\min }\\\\limits_x f\\\\left( {x,y} \\\\right) \\\\le \\\\mathop {\\\\max }\\\\limits_y f\\\\left( {{x^*},y} \\\\right)\\\\]. Now substituting in the expression from above we have:\\\\[\\\\mathop {\\\\max }\\\\limits_y \\\\mathop {\\\\min }\\\\limits_x f\\\\left( {x,y} \\\\right) \\\\le \\\\mathop {\\\\min }\\\\limits_x \\\\mathop {\\\\max }\\\\limits_y f\\\\left( {x,y} \\\\right)\\\\], \\n\\n \\n\\n \\n\\n\\n\\n\\n\\n\\n\\n An open question remains regarding when the order of min and max can be exchanged.\\n\\n\\n\\n\\nUseful General Knowledge\\n\\n\\n\\nThis section contains general knowledge and tricks about different things that are useful\\n\\n\\n\\n\\n\\n\\n\\nUse Cholesky to Sample from Gaussian\\n\\n\\n\\nThere are a number of python functions that can be used to sample from a multivariate Gaussian. One technique is to sample from a standard Gaussian (i.e. zero mean, identity covariance), and then transform the sample such that it resembles being sampled from a Gaussian with mean m and covariance S. This can be achieved as follows (taken from “Gaussian Processes for Machine Learning slide 9”)\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFix Printer Toner Issue\\n\\nThis is a set of instructions to fix the issue that I experienced with the brother laser printer. The issue is that the printer would not function because it indicated that the toner needed to be replaced.\\n\\n\\n\\nhttps://forums.macresource.com/read.php?1,1039867,1039867,quote=1\\n\\n\\xa0\\n\\n\\\\\\\\The following method below worked for HL-2140. THANKS! I routinely check my levels myself and am using a starter cartridge on 3rd refill. Still looks great and have not bought flag gear. I needed a way to use my printer without buying a new cartridge (yet)and this worked for me.\\n\\n1. Open the front cover and leave open while completing the following steps.\\n\\n2. Turn the printer off.\\n\\n3. Hold the 'go' button while turning the printer on. All panel lights should be on.\\n\\n4. Release the 'go' button.\\n\\n5. Press the 'go' button 2 times.\\n\\n6. Pause.\\n\\n7. Press the 'go' button 5 times.\\n\\n8. The toner light should be off.\\n\\n9. The paper light should be on or flashing.\\n\\n10. Close cover. The ready light should be the only light on.\\n\\n\\n\\n\\n\\n\\nLessons\\n\\n\\n\\nThis section describes lessons that I have learned through life.\\n\\n\\n\\nPhD\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGeneral\\n\\n\\n\\n\\n\\nEating out can make you feel crappy\\n\\nLesson date\\n\\nOctober 5, 2021\\n\\nContext\\n\\nToday I had a lab retreat at the university. I did not pack a lunch as I thought that there might be food provided. There was not. Therefore, I was forced to go and eat a pita from Extreme Pita. The pita was very salty and made me feel crappy. \\n\\nLesson learned\\n\\nIn the future I would plan to bring a lunch. If I forgot my lunch, then I would purchase a protein bar or in worse case a muffin. The salt content in the pita was simply too much.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nConcepts\\n\\n\\n\\n\\n\\nLagrangian Optimization\\n\\n\\n\\n\\n\\nIntroduction to Lagrange multipliers\\n\\n\\n\\nSee the document “A Simple Explanation of Why Lagrange Multipliers Works” for some informative pictures behind this example:\\n\\n\\n\\nSuppose that we are trying to solve the optimization problem:\\n\\n\\\\[\\\\begin{array}{l}\\\\mathop {\\\\min }\\\\limits_x f\\\\left( x \\\\right)\\\\\\\\{\\\\rm{s}}{\\\\rm{.t}}{\\\\rm{.}}\\\\,\\\\,\\\\,\\\\,g\\\\left( x \\\\right) = c\\\\end{array}\\\\]\\n\\n\\n\\nTwo key insights that are needed to understand the role of Lagrangian multipliers are described below. See the section below on “What happens if we have less constraints than variables” for more details.\\n\\nThe gradient of a function will always be orthogonal to the level curve of that function at any given point. To see why, take the first order Taylor expansion around a given point. For perturbations around the point to remain on the level curve (i.e. g(x) = g(x+dx)), the perturbation direction must be orthogonal to the gradient. Therefore, the gradient of g(x) will always be orthogonal to the tangential direction of the curve g(x) = c.\\n\\nIf f(x*) is a constrained minimum satisfying the constraint g(x*) = c, then the gradient of f(x*) is orthogonal to the level curve g(x*) = c. To see why, consider the following argument: if the gradient of f(x*) were not orthogonal to the level curve g(x*) = c, then it would be possible to move in a direction along the level curve g(x*) = c which would produce a better minimum which contradicts f(x*) being the constrained minimum (to be a constrained minimum, the directional derivative of f(x*) along the directions tangent to g(x*) = c must be nonnegative). \\n\\n\\n\\nCombining these two insights, the relationship \\\\[\\\\nabla f\\\\left( x \\\\right) = \\\\lambda \\\\nabla g\\\\left( x \\\\right)\\\\]must hold for a constrained minimum. The specific scalar value\\\\[\\\\lambda \\\\] that leads to equality is known as the Lagrangian multiplier.\\n\\nWe can encode this optimization problem in an expression called the Lagrangian: \\\\[L\\\\left( {x,\\\\lambda } \\\\right) = f\\\\left( x \\\\right) - \\\\lambda \\\\left( {\\\\nabla g\\\\left( x \\\\right) - c} \\\\right)\\\\]\\n\\nIf we take the partial derivatives of the Lagrangian with respect to the variables and set these derivatives to 0, then we get all the constraints for our optimization problem.\\n\\n\\n\\n\\n\\nConstrained Optimization\\n\\n\\n\\nThe general constrained optimization problem can be written as:\\n\\n\\n\\nWhere we have both equality and inequality constraints. Many different techniques have been developed for constrained optimization as seen here: https://en.wikipedia.org/wiki/Constrained_optimization\\n\\n\\n\\nSpecifically, for equality constraints, we can use standard Lagrangian multipliers. For inequality constraints we can use the KKT conditions which generalize the method of Lagrangian multipliers.\\n\\nA useful document describing a few different optimization examples is: “KKT Examples” and “Quora - intuitive explanation of the KKT conditions”\\n\\n\\n\\nTo handle both inequality and equality constraints we must use the generalized Lagrangian. For the generalized Lagrangian, the constrained maximization (minimization) problem is rewritten as a Lagrange function whose optimal point is a saddle point, i.e. a global maximum (minimum) over the domain of the choice variables and a global minimum (maximum) over the multipliers, which is why the Karush–Kuhn–Tucker theorem is sometimes referred to as the saddle-point theorem. Note that a saddle point is a solution to a mini max problem.\\n\\n\\n\\nDuality and KKT\\n\\n\\n\\nA fantastic resource for Lagrangian duality and KKT conditions is “CS229 - Convex Optimization Overview”. The following will express a summary of what I took from this document.\\n\\n\\n\\nSuppose that we have an unconstrained convex optimization problem. To solve this problem, we can simply take the gradient of the function, equate it to zero, and solve for the values that satisfy this system of equations. Since the function is convex, any minimum will be the global minimum. \\n\\n\\n\\nNow suppose that we have a constrained convex optimization problem where we have either equality or inequality constraints. We can describe this optimization problem using a Lagrangian. A Lagrangian is a function of the variables we are trying to optimize (called the primal variables) and another set of variables which help to enforce the constraints (called the dual variables or Lagrangian multipliers). The key intuition for the Lagrangian is that there exist values for the dual variables such that the unconstrained optimization of the Lagrangian with respect to the primal variables is equivalent to the original constrained optimization problem.\\n\\n\\n\\nThe primal problem is defined as follows\\n\\n\\n\\nIn the primal problem, if x does not satisfy the constraints, then the inner max operation returns infinite. As such we can view the primal problem as describing the original objective and a barrier function that forces x to meet the constraints. This is shown below\\n\\n \\n\\n\\n\\nThe power of Lagrangian duality is that we can flip the min and max around to form a new optimization problem called the dual problem:\\n\\n\\n\\n\\n\\nFor any pair of primal and dual problems we have the result that for the optimal solutions d* <= p*. What this means is that the optimal solution to the dual problem sets a lower bound for the optimal primal solution. This property is known as weak duality.\\n\\n\\n\\nIn the cases that a primal and dual problem specify a set of constraint qualification then we have d* = p*. This is known as strong duality and implies that the two optimization problems have the same optimal solution. One of these conditions is known as Slater’s condition.\\n\\n\\n\\nThe Lagrangian can be generalized to handle inequality constraints. The generalized Lagrangian is defined so that the optimal solution is a saddle point where the Lagrangian is maximized with respect to the dual variables and minimized with respect to the primal variables. The resulting constraints on the dual and primal variables at this saddle point are known as the KKT conditions. The KKT conditions specify 4 conditions that must be satisfied for a primal and dual solution to be optimal. 1) primal feasibility describes the primal region which meets the constraints, 2) dual feasibility specifies the dual variable region which meets the constraints, 3) complementary slackness is a constraint for the inequalities which specifies that the solution must be either on the boundary or inside the boundary, 4) Lagrangian stationarity which states that the gradient of the Lagrangian with respect to the primal variables equals 0.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nQuestion I have had\\n\\n\\n\\n\\n\\nWhat happens if we have less constraints than variables\\n\\n\\n\\nTo answer this question, we must explain how constraints affect the optimization search space. Let us start by considering the totally unconstrained optimization problem:\\n\\n\\\\[\\\\mathop {\\\\min }\\\\limits_{x,y,z} f\\\\left( {x,y,z} \\\\right)\\\\]\\n\\nIn this case, the search space is the complete domain of the function \\\\[f\\\\left( {x,y,z} \\\\right)\\\\]. As such, since we have 3 variables (x, y, z), our search manifold (assuming the function behaves nicely) has 3 degrees of freedom.\\n\\n\\n\\n\\n\\nNow let us add in one constraint such that the new optimization problem is as follows:   \\n\\n\\\\[\\\\begin{array}{l}\\\\mathop {\\\\min }\\\\limits_{x,y,z} f\\\\left( {x,y,z} \\\\right)\\\\\\\\{\\\\rm{s}}{\\\\rm{.t}}{\\\\rm{.}}\\\\,\\\\,g\\\\left( {x,y,z} \\\\right) = 0\\\\end{array}\\\\]\\n\\nIn this case, the equality constraint specified by \\\\[g\\\\left( {x,y,z} \\\\right) = 0\\\\] reduces the dimension of the search space from 3 degrees of freedom to 2 degrees of freedom. In other words, by introducing this constraint, our search space is now a 2D manifold.\\n\\n\\n\\nTo understand why Lagrangian stationarity implies an optimal solution to this optimization problem, consider the following argument. Suppose that we have an \\\\[\\\\left( {x,y,z} \\\\right)\\\\] pair that is on the 2D manifold described by the constraint\\\\[g\\\\left( {x,y,z} \\\\right) = 0\\\\]and under this constraint minimizes \\\\[f\\\\]. Consider the tangent plane to the level set \\\\[g\\\\left( {x,y,z} \\\\right) = 0\\\\]at this optimal point \\\\[\\\\left( {x,y,z} \\\\right)\\\\]. Since this plane will have two dimensions, we can define the basis vectors that span the plane as \\\\[\\\\left\\\\{ {u,v} \\\\right\\\\}\\\\]. If \\\\[\\\\left( {x,y,z} \\\\right)\\\\] is a constrained minimum for\\\\[f\\\\], then the directional derivative of \\\\[f\\\\]when moving along any infinitesimal direction in the tangent plane must be nonnegative (or else \\\\[\\\\left( {x,y,z} \\\\right)\\\\] would not be the constrained minimum). Suppose we compute the directional derivative along the tangent plane for vector \\\\[h = {\\\\lambda _1}u\\\\,du + {\\\\lambda _2}v\\\\,dv\\\\]. Then we have \\\\[{D_h}f\\\\left( {x,y,z} \\\\right) = \\\\nabla f\\\\left( {x,y,z} \\\\right) \\\\cdot \\\\left( {{\\\\lambda _1}u\\\\,du + {\\\\lambda _2}v\\\\,dv} \\\\right) \\\\ge 0\\\\]. Since this must hold for all values of \\\\[{\\\\lambda _1}\\\\]and \\\\[{\\\\lambda _2}\\\\], it follows that \\\\[\\\\nabla f\\\\left( {x,y,z} \\\\right) \\\\cdot u = 0\\\\]and \\\\[\\\\nabla f\\\\left( {x,y,z} \\\\right) \\\\cdot v = 0\\\\]. To see why, pick \\\\[{\\\\lambda _1} =  - 1,{\\\\lambda _2} = 0\\\\]and then\\\\[{\\\\lambda _1} = 1,{\\\\lambda _2} = 0\\\\]and solve the inequality. Hence, we see that \\\\[\\\\nabla f\\\\left( {x,y,z} \\\\right)\\\\]is orthogonal to the tangent plane. \\n\\n\\n\\nNow turning our attention to the constraint function\\\\[g\\\\]. If we take the first order Taylor series expansion of \\\\[g\\\\]around \\\\[\\\\left( {x,y,z} \\\\right)\\\\] then we get:\\n\\n\\n\\nIf we pick h to be along the tangent plane as described above where\\\\[h = {\\\\lambda _1}u\\\\,du + {\\\\lambda _2}v\\\\,dv\\\\], then \\\\[g\\\\left( {x + {h_x},y + {h_y},z + {h_z}} \\\\right) \\\\cong 0\\\\]since the tangent plane is tangent to the 2D manifold described by \\\\[g\\\\left( {x,y,z} \\\\right) = 0\\\\]and infinitesimal movements along the tangent plane from the point \\\\[\\\\left( {x,y,z} \\\\right)\\\\]will also be on the 2D manifold. Hence, we can write the following:\\n\\n\\n\\nTherefore, the gradient \\\\[\\\\nabla g\\\\left( {x,y,z} \\\\right)\\\\]is orthogonal to the 2D manifold described by\\\\[g\\\\left( {x,y,z} \\\\right) = 0\\\\]. Finally, since the unconstrained space has dimension 3 and the tangent plane has dimension 2, the orthogonal space to the tangent plane must have dimension 1. Thus, it follows that\\\\[\\\\nabla f\\\\left( {x,y,z} \\\\right)\\\\]and \\\\[\\\\nabla g\\\\left( {x,y,z} \\\\right)\\\\]must be parallel and scalar multiples of each other, which is the Lagrangian stationarity condition.\\n\\n\\n\\n\\n\\n\\n\\nOk, now let us add in one more constraint such that the new optimization problem is as follows:   \\n\\n\\\\[\\\\begin{array}{l}\\\\mathop {\\\\min }\\\\limits_{x,y,z} f\\\\left( {x,y,z} \\\\right)\\\\\\\\{\\\\rm{s}}{\\\\rm{.t}}{\\\\rm{.}}\\\\,\\\\,g\\\\left( {x,y,z} \\\\right) = 0\\\\\\\\d\\\\left( {x,y,z} \\\\right) = 0\\\\end{array}\\\\]\\n\\n\\n\\nSee the document “An Example With Two Lagrange Multipliers” for a good explanation of this situation and why the Lagrangian stationarity condition holds. However, in a nutshell, having two constraint functions now reduces the dimension for our manifold to a 1D curve. The same argument follows from above that \\\\[\\\\nabla f\\\\left( {x,y,z} \\\\right)\\\\]must be orthogonal to the direction of the 1D manifold curve and that \\\\[\\\\nabla g\\\\left( {x,y,z} \\\\right)\\\\] and \\\\[\\\\nabla d\\\\left( {x,y,z} \\\\right)\\\\]are also orthogonal to the direction of the 1D manifold curve. However, the 1D manifold now has an orthogonal space that is 2D. Therefore, assuming that \\\\[\\\\nabla g\\\\left( {x,y,z} \\\\right)\\\\] and \\\\[\\\\nabla d\\\\left( {x,y,z} \\\\right)\\\\]are not parallel, they describe a basis for the orthogonal space of the 1D manifold curve, and therefore we can describe \\\\[\\\\nabla f\\\\left( {x,y,z} \\\\right)\\\\]as a linear combination of these two basis vectors such that \\\\[\\\\nabla f\\\\left( {x,y,z} \\\\right) = \\\\lambda \\\\nabla g\\\\left( {x,y,z} \\\\right) + \\\\mu \\\\nabla d\\\\left( {x,y,z} \\\\right)\\\\]. This equation describes the Lagrangian stationarity condition for this example.\\n\\n\\n\\n\\n\\nFinally, let us add one more constraint such that the new optimization problem is as follows:\\n\\n\\\\[\\\\begin{array}{l}\\\\mathop {\\\\min }\\\\limits_{x,y,z} f\\\\left( {x,y,z} \\\\right)\\\\\\\\{\\\\rm{s}}{\\\\rm{.t}}{\\\\rm{.}}\\\\,\\\\,g\\\\left( {x,y,z} \\\\right) = 0\\\\\\\\d\\\\left( {x,y,z} \\\\right) = 0\\\\\\\\s\\\\left( {x,y,z} \\\\right) = 0\\\\end{array}\\\\]\\n\\nIn this case, the dimension of the manifold search surface drops to 0 dimensions and would simply be points in space that are satisfied by all three constraints. Hence in this case, no Lagrangian is needed.  \\n\\n   \\n\\nWhat happens if we have inequality constraints\\n\\n\\n\\nThe standard Lagrangian multiplier technique only allows equality constraints. Therefore, to solve optimization problems containing inequality constraints, the method of Lagrangian multipliers must be generalized using the KKT conditions. The KKT conditions enable the formulation of a Lagrangian in terms of primal and dual variables where the optimization is solved by minimizing the primal variables and maximizing the dual variables. Hence, the solution to the optimization problem is a saddle point of the generalized Lagrangian.\\n\\nFor more detail see documents: “CSC 411 - Lagrange Multipliers” and “Quora - intuitive explanation of the KKT conditions”, and the Wikipedia article https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions\\n\\n\\n\\nSuppose that we are trying to solve the following optimization problem:\\n\\n\\\\[\\\\begin{array}{l}\\\\mathop {\\\\min }\\\\limits_x f\\\\left( x \\\\right)\\\\\\\\{\\\\rm{s}}{\\\\rm{.t}}{\\\\rm{.}}\\\\,\\\\,g\\\\left( x \\\\right) \\\\le 0\\\\end{array}\\\\] \\n\\nThere are two possible cases that solve this problem\\n\\nThe optimal solution \\\\[{x^*}\\\\]is inside the constraint region such that\\\\[g\\\\left( {{x^*}} \\\\right) < 0\\\\]. In this case, this constraint can be considered inactive. The solution can be found by simply solving \\\\[\\\\nabla f\\\\left( {{x^*}} \\\\right) = 0\\\\]. In this region, the dual variable can be set to 0, such that\\\\[\\\\alpha  = 0\\\\]. \\n\\nThe optimal solution\\\\[{x^*}\\\\] lies on the constraint boundary such that \\\\[g\\\\left( {{x^*}} \\\\right) = 0\\\\]. In this case, we can apply the standard Laplacian stationarity constraint such that\\\\[\\\\nabla f\\\\left( {{x^*}} \\\\right) - \\\\alpha \\\\nabla g\\\\left( {{x^*}} \\\\right) = 0\\\\]. However, we can go further than this since we know that \\\\[\\\\alpha  > 0\\\\]. To see why, suppose that \\\\[\\\\nabla f\\\\left( {{x^*}} \\\\right)\\\\]was in the same direction as \\\\[\\\\nabla g\\\\left( {{x^*}} \\\\right)\\\\], then it is possible to move in a direction that simultaneously reduces g while reducing f. Hence, the constrained minimum would exist inside the constraint region and not on the boundary. Therefore, it follows that \\\\[\\\\nabla f\\\\left( {{x^*}} \\\\right)\\\\]must be in the opposite direction as \\\\[\\\\nabla g\\\\left( {{x^*}} \\\\right)\\\\]and \\\\[\\\\alpha  > 0\\\\].\\n\\n\\n\\nNote that in both cases we have the situation where \\\\[\\\\alpha g\\\\left( {{x^*}} \\\\right) = 0\\\\]. This relationship is part of the KKT conditions and is known as complementary slackness. Complementary slackness ensures that the optimal solution is either inside the constraint region or lies on the constraint boundary. \\n\\n\\n\\nWe can write down all the constraints necessary to two cases describe above as follows:\\n\\nPrimal feasibility: \\\\[g\\\\left( {{x^*}} \\\\right) \\\\le 0\\\\]\\n\\nDual feasibility: \\\\[\\\\alpha  \\\\ge 0\\\\]\\n\\nComplementary slackness: \\\\[\\\\alpha g\\\\left( {{x^*}} \\\\right) = 0\\\\]\\n\\nLagrangian stationarity: \\n\\n\\n\\nThese conditions are collectively known as the KKT conditions and if they are satisfied, guarantee that the solution is optimal for the equality and inequality constraints applied. \\n\\n\\n\\n\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEM Algorithm\\n\\n\\n\\n\\n\\nSummary\\n\\n\\n\\nKey concept of EM algorithm: \\n\\nSuppose we observe some data x and have a probabilistic model with latent variables z and parameters theta. One way we could estimate the parameters would be to maximize the log likelihood\\\\[L\\\\left( \\\\theta  \\\\right) = \\\\log p\\\\left( {x;\\\\theta } \\\\right)\\\\]. However, since our model connects the parameters to the observations through the latent variables, we must calculate the log likelihood using the joint distribution\\\\[L\\\\left( \\\\theta  \\\\right) = \\\\log p\\\\left( {x;\\\\theta } \\\\right) = \\\\log \\\\sum\\\\limits_z {p\\\\left( {x,z;\\\\theta } \\\\right)} \\\\]. At this point, we could try to maximize \\\\[L\\\\left( \\\\theta  \\\\right)\\\\], however for many problems that include exponential distributions (e.g. Gaussian), computing the sum of logs is much nicer than computing the log of sums, for both the math analysis and the numerical calculation stability. Therefore, it would be nice if we could swap the order of the log and sum. One way to do this is to use Jensen’s inequality to define the ELBO which is a lower bound on the log likelihood. The ELBO is a function of the parameters \\\\[\\\\theta \\\\] and the distribution for the latent variables Q. Therefore, the optimal parameters \\\\[\\\\theta \\\\]are found by maximizing ELBO over\\\\[\\\\theta \\\\] and Q. It can be shown that for a fixed \\\\[\\\\theta \\\\], the latent distribution which maximizes ELBO is \\\\[Q\\\\left( z \\\\right) = p\\\\left( {z|x;\\\\theta } \\\\right)\\\\]. Hence, one way of maximizing the ELBO is to alternate between two steps: 1) fix\\\\[\\\\theta \\\\]and pick \\\\[Q\\\\left( z \\\\right) = p\\\\left( {z|x;\\\\theta } \\\\right)\\\\], and 2) fix Q and maximize ELBO with respect to \\\\[\\\\theta \\\\]. This process is called Expectation Maximization and is guaranteed to converge as the ELBO will monotonically increase each iteration. Note that it is not guaranteed that the EM algorithm will find the global optima if the log likelihood function itself has multiple local optima. \\n\\n\\n\\n\\n\\n\\n\\nA very useful reference for the EM algorithm is the document: “CS229 - The EM algorithm” The math in this discussion will be based off the framework defined in that document.\\n\\n\\n\\nTo motivate the EM algorithm, consider the following situation described in the document “What is the expectation maximization algorithm”. Suppose we have a simple coin-flipping experiment where we are given a pair of coins A and B of unknown biases theta_A and theta_B respectively. Our goal is to estimate these unknown biases using data collected from performing coin flips. During the experiment, suppose that we keep track of two vectors: 1) x – a vector containing 0 or 1 depending on whether the flip resulted in heads or tails respectively, and 2) z - a vector containing A or B depending on the identity of the coin being flipped. A simple intuitive way to estimate the unknown parameters is simply to compute the following:  \\n\\n\\n\\nIt turns out that this intuitive guess is, in fact, the maximum likelihood solution for this problem. In this way, these estimated parameters maximize the joint distribution \\\\[P\\\\left( {x,z;\\\\theta } \\\\right)\\\\].\\n\\n\\n\\nNow let us consider a much more challenging variant of this parameter estimation problem which is that we are given the recorded flip results (the vector x), but we are not given the identity of the coin (the vector z). In this situation, we now have incomplete data, and we refer to z as hidden variables or latent factors. As such, we cannot compute the proportion of heads directly as we don’t know the identity of each flip. However, if we had a way of “guessing” the hidden variables by assigning a guess for each flip identity, then we could estimate the parameters using maximum likelihood as before.\\n\\n\\n\\nOne iterative scheme for such a process could work as follows:\\n\\nWe start with some initial parameter estimates (theta_A, theta_B)\\n\\nWe determine the most likely coin identity for each coin flip given the data and our parameter estimates\\n\\nThen we assume that these estimated coin flip identities are correct, and we apply maximum likelihood to get a new estimate for the parameters (theta_A, theta_B)\\n\\nWe iterate steps 2 and 3 until convergence\\n\\n\\n\\nThe EM algorithm is a refinement on the basic idea described above. However, instead of picking the single best coin flip identities, we compute the conditional probability distribution for each coin flip identity conditioned on the data and the current parameter estimates. Intuitively, we can think of the EM algorithm as a modified version of maximum likelihood which considers all possible completions of the training data (i.e. all possible coin flip identity assignments) and weighs them depending on their conditional probability. \\n\\n\\n\\nIn summary, the EM algorithm alternates between the steps of: 1) guessing a probability distribution over completions of missing data given the current model (known as the E-step), and 2) using these completions to re-estimate the model parameters (known as the M-step).\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nKey insights for EM algorithm\\n\\nThe EM algorithm is a great candidate for parameter estimation problems where solving the original maximum likelihood problem is hard or intractable, but the problem becomes much easier to solve if we could introduce some latent variables\\n\\nWe can view the EM algorithm as alternating between: 1) estimating the conditional distribution for the latent variables using our current estimate of the parameters, 2) using this conditional distribution to re-estimate the parameter values.\\n\\nBy using Jensen’s inequality over the log likelihood, we can come up with a lower bound for the likelihood function called the evidence lower bound (ELBO). The log likelihood will always >= than the ELBO. Therefore, by optimizing the ELBO we optimize the lower bound on the log likelihood.\\n\\nIf we pick the distribution over the latent variables to equal the conditional probability of the latent variables given the parameters, then the ELBO will equal the log likelihood for that specific parameter value. Then, by optimizing the ELBO for the parameters, we can find a parameter choice which increases the lower bound for the log likelihood. This process is shown in the figure below. \\n\\nTo justify that the EM algorithm converges we can show that the EM algorithm monotonically improves the log-likelihood each iteration.\\n\\nIn summary, the ELBO is always a lower bound on the likelihood function. Therefore, for a given parameter choice, we can pick the latent distribution to maximize the ELBO for the current parameters, and then keep the distribution fixed and optimize the parameters over the ELBO. \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHere is a rough high-level derivation of the EM algorithm for a single data point x:\\n\\nStart with the likelihood function that we want to maximize\\\\[p\\\\left( {x;\\\\theta } \\\\right)\\\\]for a set of parameters theta and observed data x \\n\\nWrite this likelihood function as a marginalization over the latent variables\\\\[p\\\\left( {x;\\\\theta } \\\\right) = \\\\sum\\\\limits_z {p\\\\left( {x,z;\\\\theta } \\\\right)} \\\\]\\n\\nWrite the log likelihood by taking the log of both sides \\\\[L\\\\left( \\\\theta  \\\\right) = \\\\log p\\\\left( {x;\\\\theta } \\\\right) = \\\\log \\\\sum\\\\limits_z {p\\\\left( {x,z;\\\\theta } \\\\right)} \\\\]\\n\\nUse Jensen’s inequality to write the log likelihood as an upper bound of the ELBO. Notice that the distribution Q is considered to be a fixed parameter \\\\[\\\\log p\\\\left( {x;\\\\theta } \\\\right) \\\\ge \\\\sum\\\\limits_z {Q\\\\left( z \\\\right)\\\\log \\\\frac{{p\\\\left( {x,z;\\\\theta } \\\\right)}}{{Q\\\\left( z \\\\right)}}}  = ELBO\\\\left( {x;Q,\\\\theta } \\\\right)\\\\]\\n\\nWe notice that the inequality described above becomes an equality if we pick \\\\[Q\\\\left( z \\\\right) = p\\\\left( {z|x;\\\\theta } \\\\right)\\\\]for a specific setting of the parameters\\\\[\\\\theta \\\\]. \\n\\nGiven Jensen’s inequality from step 4, we know that the ELBO is a lower bound on the log likelihood for any values of Q, theta, and x. Therefore, we can construct an iterative algorithm that works as follows: 1) start with some initial values for theta, 2) determine the conditional distribution Q such that the ELBO has equality with the log likelihood using the current value of theta, 3) optimize the ELBO to find a theta that increases the ELBO value and hence increases the lower bound of the log likelihood, 4) iterate until convergence\\n\\nWe can justify that the EM algorithm converges by noting that the EM algorithm monotonically improves the log likelihood at each iteration.\\n\\nAnother valuable interpretation of the ELBO is by writing it as follows:  \\\\[ELBO\\\\left( {x;Q,\\\\theta } \\\\right) = \\\\log p\\\\left( {x;\\\\theta } \\\\right) - {D_{KL}}\\\\left( {Q\\\\left( z \\\\right)||p\\\\left( {z|x;\\\\theta } \\\\right)} \\\\right)\\\\]. From this form, we clearly see that the ELBO is maximized when the KL divergence is 0 which happens when Q is picked to be the posterior distribution for z.  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLet’s return to the example of the coin flips described at the beginning of this section where our goal was to estimate the biases of the coins without knowing the coin flip identity. As mentioned before, during the experiment, suppose that we keep track of two vectors: 1) x – a vector containing 0 or 1 depending on whether the flip resulted in heads or tails respectively, and 2) z – a latent vector containing 0 or 1 depending on the identity of the coin being flipped being A or B respectively. \\n\\nIn this case, \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nProof of Jensen’s inequality\\n\\n\\n\\nJensen’s inequality states that for any convex function we have \\\\[E\\\\left[ {f\\\\left( X \\\\right)} \\\\right] \\\\le f\\\\left( {E\\\\left[ X \\\\right]} \\\\right)\\\\]. There are several proofs of this inequality that are contained here: https://en.wikipedia.org/wiki/Jensen%27s_inequality#Proof_1_(finite_form)\\n\\n\\n\\nNone of them are super simple, and so I will not present any formal proof here. However, the intuition is important and can be described using the figure below. In the figure, a convex function is shown and we consider a random variable X that can take two possible values: a or b with equal probability. As shown in the figure, the value \\\\[E\\\\left[ {f\\\\left( X \\\\right)} \\\\right]\\\\]represents the y-axis midpoint of the line segment connecting a and b, while \\\\[f\\\\left( {E\\\\left[ X \\\\right]} \\\\right)\\\\]represents the function being evaluated at the x-axis midpoint between a and b. We see that because f is convex, \\\\[E\\\\left[ {f\\\\left( X \\\\right)} \\\\right] \\\\le f\\\\left( {E\\\\left[ X \\\\right]} \\\\right)\\\\].  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nVariational Methods\\n\\n\\n\\nConsider the goal of trying to learn the distribution \\\\[p\\\\left( {x;\\\\theta } \\\\right)\\\\]for a set of complex high dimensional data (e.g. images of people’s faces) that we can use to sample new data. It is obvious, that the distribution of such data is far too complicated to be accurately described by a simple combination of distributions that are nice to work with (e.g. scalar mean mixture of Gaussians). Therefore, an important question is how can we represent this distribution? One strategy is to consider an image as being generated from a simple latent space representation (e.g. Gaussian) that undergoes a complex transformation (e.g. neural network) to produce the final image. For example, suppose that we have a latent space defined as \\\\[z\\\\~N\\\\left( {0,{I_{k \\\\times k}}} \\\\right)\\\\], and we use samples from the latent space to generate data according to \\\\[x|z\\\\~N\\\\left( {g\\\\left( {z;\\\\theta } \\\\right),{\\\\sigma ^2}{I_{d\\\\, \\\\times d}}} \\\\right)\\\\]where \\\\[g\\\\left( {z;\\\\theta } \\\\right)\\\\]is a neural network that takes as input the latent space vector and has weights \\\\[\\\\theta \\\\]. We can compute the likelihood of an image by expanding the joint distribution in terms of the latent space vector\\\\[p\\\\left( {x;\\\\theta } \\\\right) = \\\\sum\\\\limits_z {p\\\\left( {x,z;\\\\theta } \\\\right)}  = \\\\sum\\\\limits_z {p\\\\left( {x|z;\\\\theta } \\\\right)p\\\\left( z \\\\right)} \\\\]\\n\\nIf we could learn the parameters \\\\[\\\\theta \\\\] which maximize the likelihood of the data \\\\[p\\\\left( {x;\\\\theta } \\\\right)\\\\], then we would have an efficient way to sample new images. This could be achieved by first sampling the latent vector from the Gaussian\\\\[z\\\\~N\\\\left( {0,{I_{k \\\\times k}}} \\\\right)\\\\], and then sampling the image data from the conditional distribution \\\\[x|z\\\\~N\\\\left( {g\\\\left( {z;\\\\theta } \\\\right),{\\\\sigma ^2}{I_{d\\\\, \\\\times d}}} \\\\right)\\\\].  \\n\\n\\n\\nGiven that we are trying to maximize the likelihood of a distribution that depends on latent variables, we are in a position to apply the same methodology as for the EM algorithm to estimate the parameters\\\\[\\\\theta \\\\]. Recall that the ELBO(Q, theta) is a lower bound for the log likelihood for all values of Q and\\\\[\\\\theta \\\\]. Recall that Q is a distribution for the latent variables z. Hence, we can solve for the \\\\[\\\\theta \\\\] which maximizes the log likelihood by solving the maximization problem:\\n\\n\\\\[\\\\mathop {\\\\max }\\\\limits_Q \\\\mathop {\\\\max }\\\\limits_\\\\theta  ELBO\\\\left( {Q,\\\\theta } \\\\right)\\\\]\\n\\nIn the EM algorithm, this maximization was solved iteratively using the E and M steps. In the E-step, the distribution Q was picked to maximize the ELBO for a given\\\\[\\\\theta \\\\]. This was achieved by setting \\\\[Q\\\\left( z \\\\right) = p\\\\left( {z|x;\\\\theta } \\\\right)\\\\]so that the ELBO touches the log likelihood. Then in the M-step, \\\\[\\\\theta \\\\] is optimized to maximize the ELBO for fixed Q. \\n\\n    \\n\\nIn the context of maximizing the log likelihood of our image distribution\\\\[p\\\\left( {x;\\\\theta } \\\\right)\\\\], computing the true posterior distribution \\\\[p\\\\left( {z|x;\\\\theta } \\\\right)\\\\]is not tractable since this involves computing \\\\[p\\\\left( {x;\\\\theta } \\\\right)\\\\]as seen from writing the posterior using Bayes’ rule\\\\[p\\\\left( {z|x;\\\\theta } \\\\right) = \\\\frac{{p\\\\left( {x|z;\\\\theta } \\\\right)p\\\\left( {z;\\\\theta } \\\\right)}}{{p\\\\left( {x;\\\\theta } \\\\right)}}\\\\] . Therefore, instead of solving for the exact posterior distribution, we aim to find a close approximation. The goal is to fix a search space of potential distributions for Q and find the Q within this search space that maximizes the ELBO. This process of approximating the posterior distribution is known as variational inference.   \\n\\n\\n\\nThe next question is what form of Q allows us to efficiently maximize the objective above? One possible choice is to let the Q be a Gaussian described as \\\\[Q = N\\\\left( {q\\\\left( {x;\\\\phi } \\\\right),diag{{\\\\left( {v\\\\left( {x;\\\\varphi } \\\\right)} \\\\right)}^2}} \\\\right)\\\\], where q and v are both neural networks with parameters \\\\[\\\\phi \\\\]and \\\\[\\\\varphi \\\\]respectively and are commonly referred to as the encoder (likewise \\\\[g\\\\left( {z;\\\\theta } \\\\right)\\\\]from above is called the decoder). Notice that we are forcing the covariance to be a diagonal matrix which means that each of the components of the distributions is independent. This is known as the mean field assumption.     \\n\\n\\n\\nTo optimize the parameters we can perform gradient ascent as follows:\\n\\n\\n\\n\\n\\nHowever, we run into a challenge: evaluating the ELBO involves an expectation over a distribution that depends on \\\\[\\\\phi \\\\]and \\\\[\\\\varphi \\\\]. Therefore, evaluating the gradient \\n\\n\\n\\n\\n\\n\\nHMMs\\n\\n\\n\\n\\n\\nSummary\\n\\n\\n\\nA very useful reference for HMMs is the document: “CS229 - Hidden Markov Models Fundamentals” The math in this discussion will be based off the framework defined in that document.\\n\\nAnother useful document is “Jurafsky - Hidden Markov Models”.\\n\\n\\n\\n\\n\\nSuppose we have a set of possible states for our system and a bunch of data describing the observed transitions between states. Let us assume the Markov assumptions for this system: 1) limited horizon assumption – the probability of being a state at time t only depends on the state at time t-1, 2) stationary process assumption – the conditional distribution over the next state given the current state does not change with time.\\n\\nGiven the state transition data we have observed, how can we estimate the conditional transition probabilities?\\n\\nOne method to approach this estimation problem is to compute the maximum likelihood solution. In this case we can optimize the probability parameters to maximize the likelihood of the observed data sequence. Given the constraint that the probabilities sum to 1, we can use Lagrangian multipliers to perform the optimization. The resulting solution for each transition probability is simply the relative frequency. For example, the probability for the transition between states S1 -> S2 would be computed as the number of occurrences of the transition from S1 -> S2, divided by the total number of occurrences of transitions originating from S1. \\n\\n\\n\\nThe main motivation for Hidden Markov Models stems from the fact that in many real-world problems we do not observe the state directly, but rather a probabilistic function of the state. For example, suppose we have a robot equipped with a camera sensor that is navigating through a building. To perform its tasks, the robot must know its position, however we can’t determine the position directly, we can only infer the position from the data provided by the camera, and the camera sensor can only provide a probabilistic function of the position. We can use HMMs to infer what the distribution for the underlying hidden state (in this case the robot’s position) conditioned on the observed measurements (in this case the camera position measurements).\\n\\n\\n\\nHMMs rely on the output independence assumption which states that the probability of an observed output conditioned on a hidden state is independent of all prior observations and hidden states.\\n\\n\\n\\nThere are three fundamental questions that we might ask an HMM:\\n\\nWhat is the probability of a given sequence of observations assuming we know the state transition matrix A and the observation generating matrix B\\n\\nWhat is the most likely series of hidden states that would generate a given sequence of observations assuming we know the state transition matrix A and the observation generating matrix B\\n\\nHow can we learn the HMM parameters A and B given some observation data\\n\\n\\n\\n\\n\\nA critical component of working with HMMs is being able to evaluate the probability of a given sequence of observed data conditioned on the state transition matrix A and the hidden state generating output matrix B. The naïve way to compute this probability is to simply sum the probabilities over all possible hidden state sequences. However, this is extremely expensive and has time complexity O(|S|^T) where |S| is the number of states and T is the number of time steps. \\n\\nTo improve the performance, we can notice that the probability of observing a sequence with t time steps and being in state s_i at time t can be written recursively as shown below:\\n\\n\\n\\n \\\\[\\\\begin{array}{l}{\\\\rm{(Line 1)   }}{\\\\alpha _j}\\\\left( t \\\\right) = P\\\\left( {{x_1},{x_2},...,{x_t},{z_t} = {s_j};A,B} \\\\right)\\\\\\\\{\\\\rm{(Line 2)}} = \\\\sum\\\\limits_{i = 1}^{\\\\left| S \\\\right|} {P\\\\left( {{x_1},{x_2},...,{x_t},{z_t} = {s_j},{z_{t - 1}} = {s_i};A,B} \\\\right)} \\\\\\\\{\\\\rm{(Line 3)}} = \\\\sum\\\\limits_{i = 1}^{\\\\left| S \\\\right|} {P\\\\left( {{x_t}|{x_1},{x_2},...,{x_{t - 1}},{z_t} = {s_j},{z_{t - 1}} = {s_i};A,B} \\\\right)P\\\\left( {{x_1},{x_2},...,{x_{t - 1}},{z_t} = {s_j},{z_{t - 1}} = {s_i};A,B} \\\\right)} \\\\\\\\{\\\\rm{(Line 4)}} = \\\\sum\\\\limits_{i = 1}^{\\\\left| S \\\\right|} {P\\\\left( {{x_t}|{z_t} = {s_j};A,B} \\\\right)P\\\\left( {{x_1},{x_2},...,{x_{t - 1}},{z_t} = {s_j},{z_{t - 1}} = {s_i};A,B} \\\\right)} \\\\\\\\{\\\\rm{(Line 5)}} = \\\\sum\\\\limits_{i = 1}^{\\\\left| S \\\\right|} {{B_{j,x}}P\\\\left( {{x_1},{x_2},...,{x_{t - 1}},{z_t} = {s_j},{z_{t - 1}} = {s_i};A,B} \\\\right)} \\\\\\\\{\\\\rm{(Line 6)}} = \\\\sum\\\\limits_{i = 1}^{\\\\left| S \\\\right|} {{B_{j,x}}P\\\\left( {{z_t} = {s_j}|{x_1},{x_2},...,{x_{t - 1}},{z_{t - 1}} = {s_i};A,B} \\\\right)P\\\\left( {{x_1},{x_2},...,{x_{t - 1}},{z_{t - 1}} = {s_i};A,B} \\\\right)} \\\\\\\\{\\\\rm{(Line 7)}} = \\\\sum\\\\limits_{i = 1}^{\\\\left| S \\\\right|} {{B_{j,x}}P\\\\left( {{z_t} = {s_j}|{z_{t - 1}} = {s_i};A,B} \\\\right)P\\\\left( {{x_1},{x_2},...,{x_{t - 1}},{z_{t - 1}} = {s_i};A,B} \\\\right)} \\\\\\\\{\\\\rm{(Line 8)}} = \\\\sum\\\\limits_{i = 1}^{\\\\left| S \\\\right|} {{B_{j,x}}{A_{i,j}}P\\\\left( {{x_1},{x_2},...,{x_{t - 1}},{z_{t - 1}} = {s_i};A,B} \\\\right)} \\\\\\\\{\\\\rm{(Line 9)}} = \\\\sum\\\\limits_{i = 1}^{\\\\left| S \\\\right|} {{B_{j,x}}{A_{i,j}}{\\\\alpha _i}\\\\left( {t - 1} \\\\right)} \\\\end{array}\\\\] \\n\\n\\n\\nProof notes: Line 1 follows from the definition of alpha. Line 2 follows from the definition of joint probability; in this case we are summing over the latent state variable z_t-1. Line 3 follows from factoring the joint distribution. Line 4 follows from the HMM assumption of output independence. Line 5 follows from the definition of B. Line 6 follows from factoring the joint distribution. Line 7 follows from the HMM assumption of limited horizon. Line 8 follows from the definition of A. Line 9 follows from the definition of alpha.\\n\\n\\n\\nTherefore, we can solve for the alpha values recursively using dynamic programing. The final probability of the sequence is given as\\n\\n\\n\\n\\n\\nThis algorithm to compute the probability of a sequence is called the forward pass and can be computed in O(|S| * T) time complexity.\\n\\n\\n\\n\\n\\n\\n\\n\\nTransformers\\n\\n\\n\\n\\n\\nSummary\\n\\n\\n\\nVery good resources\\n\\nhttps://huggingface.co/blog/encoder-decoder\\n\\nhttps://jalammar.github.io/illustrated-transformer/\\n\\n\\n\\nVery good video source: https://www.youtube.com/watch?v=WFcH7kRNEBc&t=2839s&ab_channel=DataScienceCourses\\n\\n\\n\\n\\n\\nConsider the task of translating a sentence from English to French. For this task, we are given the sequence of words in an English sentence, and the goal is to output a sequence of French words that that carries the same underlying meaning. \\n\\nThe first naïve solution might be to learn a vector embedding representation for each word in English and French dictionaries (e.g. word2vec) and then translate each word in the English sentence into the nearest French word. However, many languages cannot be translated word to word, therefore, we must have a model that can utilize the order of the words and the relationships between the words in the sentence to perform the translation.\\n\\n\\n\\nOne solution is to use an encoder/decoder architecture to perform the translation. The key insight for this method is that the encoder takes the English words as input and produces a context vector that describes the relationships between the words in the sentence. This context vector can be used by the decoder to inform which words to generate in French. As such, we can interpret the context vector as representing the abstract concepts described by the input sentence. Historically, the encoder/decoder models have been built using RNN architectures. Each step of the encoder RNN takes in a word of the input sentence and the hidden state of the last recurrent block was outputted as the context vector. One of the main problems with this method is that it is hard to represent long-term dependencies in the context vector. A solution to the problem of long-term dependencies is to include attention in the seq2seq architecture. \\n\\n\\n\\nWhat is attention? Attention is really just a fancy name given to the process of combining information together as a weighted average that we pass forward to the network, where the weight coefficients are chosen to reflect what we want the model to focus on. Given this definition, the next question becomes: what do we want the model to focus on? The intuition behind attention is to have the model learn to put the most weight on information that is most relevant to the current task. This process is handled more formally by defining three concepts: query, key, value. The query describes what kind of information the model is looking for, the key describes what kind of information the associated value contains, and the value is the information itself that may be useful to the model. Hence, we can view the attention mechanism as calculating a weighted average of the “value” information, where the weights are determined by the softmax of the similarity between the “query” and the “keys”. We can write the expression for attention as follows:\\n\\n\\\\[attention\\\\left( {q,{\\\\bf{k}},{\\\\bf{v}}} \\\\right) = \\\\sum\\\\limits_i {similarity\\\\left( {q,{k_i}} \\\\right) \\\\times {v_i}} \\\\]\\n\\n\\n\\nTo provide a concrete example of attention, let us return to the seq2seq architecture. One way to employ attention is to compute the context vector used by the decoder by means of attention. In this way, we can consider the “query” to be the decoder’s current hidden state. We can interpret the decoder’s current hidden state as representing the current abstract concept that the decoder is trying to generate a French word for. Therefore, any hidden state information from the encoder that is similar to the current hidden state of the decoder may be useful for inference as it may contain information about the concept. Hence, for this example, we can define the “keys” to be the encoder hidden state information and also the “values” to be the encoder hidden state information. Therefore, the context vector is computed by attention through taking the weighted average of the “values” where the weights are the softmax similarity (e.g. dot product) between the “concept” and the “keys”. This process has been shown to improve the long-term dependency problem encountered by vanilla RNNs. However, RNNs still require sequential computation and have gradient vanishing/exploding challenges which limit their performance. A solution to this problem is to step away from RNNs completely and instead build an encoder/decoder architecture based on self-attention. This architecture is known as a transformer.\\n\\n\\n\\nWhat is self-attention? To motivate this concept, let us first consider the challenge of coming up with an embedding to for a sequence. For example, suppose we have the sentence “The restaurant was not that terrible”. We can come up with word embeddings using a technique such as word2vec, but how can we come up with an embedding that describes the concept of the complete sentence? Self-attention is a way to achieve this. Essentially, it is a method to come up with better embeddings. As described above, attention works by defining three concepts: query, key, value. The query describes what kind of information the model is looking for, the key describes what kind of information the associated value contains, and the value is the information itself that may be useful to the model. The “self” part of self-attention comes from the fact that in self-attention the query, key, and value information all come from the same source data. To make this more concrete let us return to the example sentence written above. To calculate the self-attention for the words in the sentence, we first project each of the word embeddings onto the query, key, and values spaces using three matrices: Q, K, and V respectively. These projection matrices are learned by the model and key intuition behind what they learn is the following: \\n\\nThe Q projection of a word represents what kind of information the model would like to have (i.e. query), that would help inform how that word should be used. For example, in the above example sentence, we have the words “not” and “terrible” which act on each other to form a double negative. Therefore, it makes sense that the query embedding for the word “not” would inform the model that the information described in the word “terrible” would be useful for its interpretation.\\n\\nThe K projection of a word represents what kind of information that word contains in its V embedding that could be used by the model. For example, the similarity between the Q embedding for the word “not” and the K embedding for the word “terrible”, describes how important it is for the model to utilize the information encoded in the V embedding for the word “terrible” to understand the concept of the word “not” in the context of the sentence. \\n\\nThe V projection of a word represents what kind of information that model would find useful about that word to inform future processing. For example, the V embedding of the word “terrible” describes the kind of information that the model would find useful about this word.\\n\\nOnce we have the Q, K, and V projections, we can compute self-attention. For example, to compute self-attention for the word “not”, we compute a weighted sum over the V projections for all words, where the weights are given by the softmax of the similarity between the Q projection for “not” and the K projections for all words in the sentence. The output of the self-attention block is a weighted combination of the V projections that can be interpreted as an embedding vector that better represents the information in the sentence relevant for the given task (e.g. language translation).       \\n\\n\\n\\nAs described above, a self-attention block is used to learn how to combine input embeddings together to compute better embeddings for a given task. Self-attention blocks form the basis of the transformer architecture. \\n\\n\\n\\nA transformer is an autoregressive model that predicts the next output token given the input sequence and the output up to that time. The transformer is composed of an encoder and a decoder. The encoder is comprised of several blocks which largely consist of a self-attention layer followed by a feed forward network. Each self-attention layer is multi-headed which means that we have multiple self-attention blocks in parallel at each layer. The key architecture of a transformer can be described as follows:\\n\\nEncoder\\n\\nThe input words are converted into word embeddings\\n\\nThe word embeddings are combined with positional encoding information to inform the network which order in the sequence each word is located\\n\\nThere are N layers of multi-head self-attention followed by a feed forward network. Each layer is normalized and also has a skip connection\\n\\nThe output of the last encoder layer is used as the context vector for the decoder\\n\\nDecoder\\n\\nSimilar to the encoder, the output words are converted into word embeddings and positionally encoded\\n\\nThere are N layers of: 1) masked multi-head self-attention, 2) encoder-decoder attention, and 3) feed forward network\\n\\nThe masked multi-head self-attention works similar to the multi-head self-attention from the encoder, with the exception that data that has not yet been generated is masked out\\n\\nThe encoder-decoder attention mechanism is used to perform attention using information from the output of the encoder. Specifically, the output of the encoder is considered to be the keys and values, and the decoder input of the encoder-decoder attention layer is considered to be the query.\\n\\nThe output of the final decoder layer is passed through a linear layer and then a final softmax to output the probability distribution over output words\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBERT\\n\\n\\n\\n\\n\\nBERT is built using transformer encoder blocks If we only have encoders, how can we build a language model? The idea is that we can mask words in the input sequence and try to predict the missing words. This can be done completely unsupervised. Word embeddings have one vector for each word, while BERT provides a context aware embedding for the word. BERT is a pretrained model that produces a rich embeddings for your words.\\n\\n\\n\\n\\n\\n\\n\\nGPT\\n\\n\\n\\nGPT is built using transformer decoder blocks. For GPT we remove the cross-attention layer since we don’t have an encoder. GPT is trained to predict the next word, given all of the previous words. GPT-3 has 175 billion parameters.\\n\\n\\n\\n\\n\\n\\nSVM\\n\\n\\n\\n\\n\\nSummary\\n\\n\\n\\nValuable references to read more on this are the following:\\n\\nCS 480 – 2019 Lecture 13\\n\\nZoya SVM Tutorial\\n\\n\\n\\nFundamentally, an SVM works by trying to find the linear decision boundary that maximizes the margin distance between classes.\\n\\n\\n\\nWe can formulate this problem with the objective of finding weights which maximize the minimum distance from points in the dataset to the hyperplane. This can be described mathematically as:\\n\\n\\n\\n\\n\\nHowever, we can view this problem from another perspective. We can visualize having two margin planes on each side of the decision boundary plane that are each a distance of \\\\[\\\\frac{1}{{\\\\left\\\\| w \\\\right\\\\|}}\\\\]away from the decision boundary plane, creating a total margin size of \\\\[\\\\frac{2}{{\\\\left\\\\| w \\\\right\\\\|}}\\\\]. In this way we can formulate the objective of finding the maximum margin as follows: maximize the margin distance \\\\[\\\\frac{2}{{\\\\left\\\\| w \\\\right\\\\|}}\\\\] (equivalent to minimizing \\\\[\\\\left\\\\| w \\\\right\\\\|\\\\]) while ensuring that all points in the dataset lie outside the margin planes. This can be written mathematically as:\\n\\n \\n\\nNote that the choice of having each margin plane a distance of \\\\[\\\\frac{1}{{\\\\left\\\\| w \\\\right\\\\|}}\\\\]from the decision boundary plane is arbitrary and could have been anything. The important part is that the margin planes are separated by some equal distance around the decision boundary plane. With this said, this constraint of the distance being \\\\[\\\\frac{1}{{\\\\left\\\\| w \\\\right\\\\|}}\\\\]is encoded in the inequality since \\\\[{w^T}\\\\left( {\\\\frac{w}{{{{\\\\left\\\\| w \\\\right\\\\|}^2}}}} \\\\right) = \\\\frac{{{w^T}w}}{{{{\\\\left\\\\| w \\\\right\\\\|}^2}}} = \\\\frac{{{{\\\\left\\\\| w \\\\right\\\\|}^2}}}{{{{\\\\left\\\\| w \\\\right\\\\|}^2}}} = 1\\\\]and \\\\[\\\\left\\\\| {\\\\frac{w}{{{{\\\\left\\\\| w \\\\right\\\\|}^2}}}} \\\\right\\\\| = \\\\frac{1}{{\\\\left\\\\| w \\\\right\\\\|}}\\\\]. Therefore,  \\\\[{y_n}{w^T}\\\\phi \\\\left( {{x_n}} \\\\right) = 1\\\\]when the data point is a distance of \\\\[\\\\frac{1}{{\\\\left\\\\| w \\\\right\\\\|}}\\\\]from the decision boundary. \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGaussian Processes\\n\\n\\n\\n\\n\\nSummary\\n\\n\\n\\nA Gaussian process can be interpreted as an infinite dimensional Gaussian random variable with mean function m(X) and covariance function k(X,X’)\\n\\nThe covariance function can be any function that when produces positive semi-definite covariance matricies for the y_i’s with any allowed set of x_i’s. See  http://www.cs.utoronto.ca/~radford/csc2541.S11/week7.pdf\\n\\nTo sample a finite number of points from a GP, we can use the fact that a finite dimensional subset of the GP results in a marginal distribution that is Gaussian with mean vector m(X) and covariance matrix k(X,X) for a given finite sample X. \\n\\nSampling is enabled by the fact that Gaussians have a really easy way to compute marginal distributions. Essentially, to compute the marginal distribution, you only keep the covariance and mean components for the values you are marginalizing on. This enables an infinite dimension GP to be marginalized to a finite dimension Gaussian distribution when sampling.\\n\\nThe conditional distribution for a Gaussian is also straightforward to compute which enables the conditioning of the GP on specific data points and can produce a conditional distribution for regression.\\n\\n\\n\\nReferences\\n\\nSee the document “Gaussian Processes for Machine Learning” for more details\\n\\n\\n\\n\\n\\n\\n\\nExercises\\n\\nFollowing slide 9 in “Gaussian Processes for Machine Learning”, write a script to sample from a GP using the squared exponential as the covariance function\\n\\nCode for this exercise is located in the python notebook GP_Exercises\\n\\nFollowing the section on Predictions from posterior here: https://peterroelants.github.io/posts/gaussian-process-tutorial/, write a script to compute the conditional distribution for a GP, using the squared exponential as the covariance function and using points from the sine function as a prior.\\n\\nCode for this exercise is located in the python notebook GP_Exercises\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCourses\\n\\n\\n\\n\\n\\nCS480 – 2019\\n\\n\\n\\nThis course introduces the basics of machine learning and does so in a way that provides nice insights. The link to the course website is here: https://cs.uwaterloo.ca/~ppoupart/teaching/cs480-spring19/index.html\\n\\n\\n\\n\\n\\nNotes\\n\\n\\n\\n\\n\\nLecture 12\\n\\nA Gaussian process is equivalent to a neural network with one hidden layer that is infinitely large.\\n\\nThe core idea with a Gaussian Process is to have a distribution over functions\\n\\nIf we assume that we have data that has no noise and we are trying to fit a function through this data.\\n\\nA gaussian process learns the Gaussian distribution of the output of a function at any point\\n\\nThe gaussians for each point must be correlated with each other with correlation stronger when the distance between points is small\\n\\nA Gaussian process is an infinite dimensional Gaussian distribution\\n\\nThe mean for the GP is a function (instead of a vector) and the covariance is a function (instead of a matrix)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nArticles\\n\\n\\n\\n\\n\\nGaussian processes (1/3) - From scratch\\n\\n\\n\\nWebsite\\n\\nhttps://peterroelants.github.io/posts/gaussian-process-tutorial/\\n\\nNotes\\n\\nWe can interpret a GP as being an infinite dimensional Gaussian random variable. Therefore to sample points from the GP, ..\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec83dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c69615",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c5a7ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9533f32e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6fdff281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"pandoc --extract-media ./myMediaFolder3 Knowledge_test.docx -o output.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b7bd86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3faeae1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\\\\\[\\\\mathop {\\\\max }\\\\limits_x \\\\mathop {\\\\min }\\\\limits_y f\\\\left(\n",
      "\n",
      "$$\\mathop {\\max }\\limits_x \\mathop {\\min }\\limits_y f\\left(\n",
      "\n",
      "\\\\\\[\\\\mathop {\\\\max }\\\\limits_x \\\\mathop {\\\\min }\\\\limits_y f\\\\left(\n",
      "\n",
      "$$\\mathop {\\max }\\limits_x \\mathop {\\min }\\limits_y f\\left(\n",
      "\n",
      "where \\\\\\[g\\\\left( x \\\\right) = \\\\mathop {\\\\min }\\\\limits_y f\\\\left(\n",
      "\n",
      "where $$g\\left( x \\right) = \\mathop {\\min }\\limits_y f\\left(\n",
      "\n",
      "### Here is a proof that for any function\\\\\\[f\\\\left( {x,y} \\\\right)\\\\\\]we have \\\\\\[\\\\mathop {\\\\max }\\\\limits_y \\\\mathop {\\\\min }\\\\limits_x f\\\\left( {x,y} \\\\right) \\\\le \\\\mathop {\\\\min }\\\\limits_x \\\\mathop {\\\\max }\\\\limits_y f\\\\left( {x,y} \\\\right)\\\\\\] \n",
      "\n",
      "### Here is a proof that for any function$$f\\left( {x,y} \\right)$$we have $$\\mathop {\\max }\\limits_y \\mathop {\\min }\\limits_x f\\left( {x,y} \\right) \\le \\mathop {\\min }\\limits_x \\mathop {\\max }\\limits_y f\\left( {x,y} \\right)$$ \n",
      "\n",
      "Suppose that \\\\\\[\\\\left( {{x\\^\\*},{y\\^\\*}} \\\\right)\\\\\\]is the optimal\n",
      "\n",
      "Suppose that $$\\left( {{x\\^\\*},{y\\^\\*}} \\right)$$is the optimal\n",
      "\n",
      "solution to \\\\\\[\\\\mathop {\\\\min }\\\\limits_x \\\\mathop {\\\\max }\\\\limits_y\n",
      "\n",
      "solution to $$\\mathop {\\min }\\limits_x \\mathop {\\max }\\limits_y\n",
      "\n",
      "f\\\\left( {x,y} \\\\right)\\\\\\], such that \\\\\\[\\\\mathop {\\\\min }\\\\limits_x\n",
      "\n",
      "f\\left( {x,y} \\right)$$, such that $$\\mathop {\\min }\\limits_x\n",
      "\n",
      "{{x\\^\\*},{y\\^\\*}} \\\\right)\\\\\\]. This implies that \\\\\\[\\\\mathop {\\\\min\n",
      "\n",
      "{{x\\^\\*},{y\\^\\*}} \\right)$$. This implies that $$\\mathop {\\min\n",
      "\n",
      "value, and since \\\\\\[{x\\^\\*}\\\\\\]is optimal, we can remove the min\n",
      "\n",
      "value, and since $${x\\^\\*}$$is optimal, we can remove the min\n",
      "\n",
      "switched such that \\\\\\[\\\\mathop {\\\\max }\\\\limits_y \\\\mathop {\\\\min\n",
      "\n",
      "switched such that $$\\mathop {\\max }\\limits_y \\mathop {\\min\n",
      "\n",
      "values then \\\\\\[\\\\mathop {\\\\min }\\\\limits_x f\\\\left( {x,y} \\\\right) \\\\le\n",
      "\n",
      "values then $$\\mathop {\\min }\\limits_x f\\left( {x,y} \\right) \\le\n",
      "\n",
      "\\\\\\[\\\\mathop {\\\\max }\\\\limits_y \\\\mathop {\\\\min }\\\\limits_x f\\\\left(\n",
      "\n",
      "$$\\mathop {\\max }\\limits_y \\mathop {\\min }\\limits_x f\\left(\n",
      "\n",
      "have:\\\\\\[\\\\mathop {\\\\max }\\\\limits_y \\\\mathop {\\\\min }\\\\limits_x\n",
      "\n",
      "have:$$\\mathop {\\max }\\limits_y \\mathop {\\min }\\limits_x\n",
      "\n",
      "\\\\\\[\\\\begin{array}{l}\\\\mathop {\\\\min }\\\\limits_x f\\\\left( x\n",
      "\n",
      "$$\\begin{array}{l}\\mathop {\\min }\\limits_x f\\left( x\n",
      "\n",
      "Combining these two insights, the relationship \\\\\\[\\\\nabla f\\\\left( x\n",
      "\n",
      "Combining these two insights, the relationship $$\\nabla f\\left( x\n",
      "\n",
      "constrained minimum. The specific scalar value\\\\\\[\\\\lambda \\\\\\] that\n",
      "\n",
      "constrained minimum. The specific scalar value$$\\lambda $$ that\n",
      "\n",
      "Lagrangian: \\\\\\[L\\\\left( {x,\\\\lambda } \\\\right) = f\\\\left( x \\\\right) -\n",
      "\n",
      "Lagrangian: $$L\\left( {x,\\lambda } \\right) = f\\left( x \\right) -\n",
      "\n",
      "\\\\\\[\\\\mathop {\\\\min }\\\\limits\\_{x,y,z} f\\\\left( {x,y,z} \\\\right)\\\\\\]\n",
      "\n",
      "$$\\mathop {\\min }\\limits\\_{x,y,z} f\\left( {x,y,z} \\right)$$\n",
      "\n",
      "\\\\\\[f\\\\left( {x,y,z} \\\\right)\\\\\\]. As such, since we have 3 variables\n",
      "\n",
      "$$f\\left( {x,y,z} \\right)$$. As such, since we have 3 variables\n",
      "\n",
      "\\\\\\[\\\\begin{array}{l}\\\\mathop {\\\\min }\\\\limits\\_{x,y,z} f\\\\left( {x,y,z}\n",
      "\n",
      "$$\\begin{array}{l}\\mathop {\\min }\\limits\\_{x,y,z} f\\left( {x,y,z}\n",
      "\n",
      "In this case, the equality constraint specified by \\\\\\[g\\\\left( {x,y,z}\n",
      "\n",
      "In this case, the equality constraint specified by $$g\\left( {x,y,z}\n",
      "\n",
      "we have an \\\\\\[\\\\left( {x,y,z} \\\\right)\\\\\\] pair that is on the 2D\n",
      "\n",
      "we have an $$\\left( {x,y,z} \\right)$$ pair that is on the 2D\n",
      "\n",
      "manifold described by the constraint\\\\\\[g\\\\left( {x,y,z} \\\\right) =\n",
      "\n",
      "manifold described by the constraint$$g\\left( {x,y,z} \\right) =\n",
      "\n",
      "0\\\\\\]and under this constraint minimizes \\\\\\[f\\\\\\]. Consider the tangent\n",
      "\n",
      "0$$and under this constraint minimizes $$f$$. Consider the tangent\n",
      "\n",
      "plane to the level set \\\\\\[g\\\\left( {x,y,z} \\\\right) = 0\\\\\\]at this\n",
      "\n",
      "plane to the level set $$g\\left( {x,y,z} \\right) = 0$$at this\n",
      "\n",
      "optimal point \\\\\\[\\\\left( {x,y,z} \\\\right)\\\\\\]. Since this plane will\n",
      "\n",
      "optimal point $$\\left( {x,y,z} \\right)$$. Since this plane will\n",
      "\n",
      "as \\\\\\[\\\\left\\\\{ {u,v} \\\\right\\\\}\\\\\\]. If \\\\\\[\\\\left( {x,y,z}\n",
      "\n",
      "as $$\\left\\{ {u,v} \\right\\}$$. If $$\\left( {x,y,z}\n",
      "\n",
      "\\\\right)\\\\\\] is a constrained minimum for\\\\\\[f\\\\\\], then the directional\n",
      "\n",
      "\\right)$$ is a constrained minimum for$$f$$, then the directional\n",
      "\n",
      "derivative of \\\\\\[f\\\\\\]when moving along any infinitesimal direction in\n",
      "\n",
      "derivative of $$f$$when moving along any infinitesimal direction in\n",
      "\n",
      "the tangent plane must be nonnegative (or else \\\\\\[\\\\left( {x,y,z}\n",
      "\n",
      "the tangent plane must be nonnegative (or else $$\\left( {x,y,z}\n",
      "\n",
      "the directional derivative along the tangent plane for vector \\\\\\[h =\n",
      "\n",
      "the directional derivative along the tangent plane for vector $$h =\n",
      "\n",
      "\\\\\\[{D_h}f\\\\left( {x,y,z} \\\\right) = \\\\nabla f\\\\left( {x,y,z} \\\\right)\n",
      "\n",
      "$${D_h}f\\left( {x,y,z} \\right) = \\nabla f\\left( {x,y,z} \\right)\n",
      "\n",
      "\\\\ge 0\\\\\\]. Since this must hold for all values of \\\\\\[{\\\\lambda\n",
      "\n",
      "\\ge 0$$. Since this must hold for all values of $${\\lambda\n",
      "\n",
      "\\_1}\\\\\\]and \\\\\\[{\\\\lambda \\_2}\\\\\\], it follows that \\\\\\[\\\\nabla f\\\\left(\n",
      "\n",
      "\\_1}$$and $${\\lambda \\_2}$$, it follows that $$\\nabla f\\left(\n",
      "\n",
      "{x,y,z} \\\\right) \\\\cdot u = 0\\\\\\]and \\\\\\[\\\\nabla f\\\\left( {x,y,z}\n",
      "\n",
      "{x,y,z} \\right) \\cdot u = 0$$and $$\\nabla f\\left( {x,y,z}\n",
      "\n",
      "\\\\right) \\\\cdot v = 0\\\\\\]. To see why, pick \\\\\\[{\\\\lambda \\_1} = -\n",
      "\n",
      "\\right) \\cdot v = 0$$. To see why, pick $${\\lambda \\_1} = -\n",
      "\n",
      "1,{\\\\lambda \\_2} = 0\\\\\\]and then\\\\\\[{\\\\lambda \\_1} = 1,{\\\\lambda \\_2} =\n",
      "\n",
      "1,{\\lambda \\_2} = 0$$and then$${\\lambda \\_1} = 1,{\\lambda \\_2} =\n",
      "\n",
      "0\\\\\\]and solve the inequality. Hence, we see that \\\\\\[\\\\nabla f\\\\left(\n",
      "\n",
      "0$$and solve the inequality. Hence, we see that $$\\nabla f\\left(\n",
      "\n",
      "Now turning our attention to the constraint function\\\\\\[g\\\\\\]. If we\n",
      "\n",
      "Now turning our attention to the constraint function$$g$$. If we\n",
      "\n",
      "take the first order Taylor series expansion of \\\\\\[g\\\\\\]around\n",
      "\n",
      "take the first order Taylor series expansion of $$g$$around\n",
      "\n",
      "\\\\\\[\\\\left( {x,y,z} \\\\right)\\\\\\] then we get:\n",
      "\n",
      "$$\\left( {x,y,z} \\right)$$ then we get:\n",
      "\n",
      "If we pick h to be along the tangent plane as described above where\\\\\\[h\n",
      "\n",
      "If we pick h to be along the tangent plane as described above where$$h\n",
      "\n",
      "= {\\\\lambda \\_1}u\\\\,du + {\\\\lambda \\_2}v\\\\,dv\\\\\\], then \\\\\\[g\\\\left( {x\n",
      "\n",
      "= {\\lambda \\_1}u\\,du + {\\lambda \\_2}v\\,dv$$, then $$g\\left( {x\n",
      "\n",
      "plane is tangent to the 2D manifold described by \\\\\\[g\\\\left( {x,y,z}\n",
      "\n",
      "plane is tangent to the 2D manifold described by $$g\\left( {x,y,z}\n",
      "\n",
      "the point \\\\\\[\\\\left( {x,y,z} \\\\right)\\\\\\]will also be on the 2D\n",
      "\n",
      "the point $$\\left( {x,y,z} \\right)$$will also be on the 2D\n",
      "\n",
      "Therefore, the gradient \\\\\\[\\\\nabla g\\\\left( {x,y,z} \\\\right)\\\\\\]is\n",
      "\n",
      "Therefore, the gradient $$\\nabla g\\left( {x,y,z} \\right)$$is\n",
      "\n",
      "orthogonal to the 2D manifold described by\\\\\\[g\\\\left( {x,y,z} \\\\right)\n",
      "\n",
      "orthogonal to the 2D manifold described by$$g\\left( {x,y,z} \\right)\n",
      "\n",
      "must have dimension 1. Thus, it follows that\\\\\\[\\\\nabla f\\\\left( {x,y,z}\n",
      "\n",
      "must have dimension 1. Thus, it follows that$$\\nabla f\\left( {x,y,z}\n",
      "\n",
      "\\\\right)\\\\\\]and \\\\\\[\\\\nabla g\\\\left( {x,y,z} \\\\right)\\\\\\]must be\n",
      "\n",
      "\\right)$$and $$\\nabla g\\left( {x,y,z} \\right)$$must be\n",
      "\n",
      "\\\\\\[\\\\begin{array}{l}\\\\mathop {\\\\min }\\\\limits\\_{x,y,z} f\\\\left( {x,y,z}\n",
      "\n",
      "$$\\begin{array}{l}\\mathop {\\min }\\limits\\_{x,y,z} f\\left( {x,y,z}\n",
      "\n",
      "argument follows from above that \\\\\\[\\\\nabla f\\\\left( {x,y,z}\n",
      "\n",
      "argument follows from above that $$\\nabla f\\left( {x,y,z}\n",
      "\n",
      "and that \\\\\\[\\\\nabla g\\\\left( {x,y,z} \\\\right)\\\\\\] and \\\\\\[\\\\nabla\n",
      "\n",
      "and that $$\\nabla g\\left( {x,y,z} \\right)$$ and $$\\nabla\n",
      "\n",
      "that is 2D. Therefore, assuming that \\\\\\[\\\\nabla g\\\\left( {x,y,z}\n",
      "\n",
      "that is 2D. Therefore, assuming that $$\\nabla g\\left( {x,y,z}\n",
      "\n",
      "\\\\right)\\\\\\] and \\\\\\[\\\\nabla d\\\\left( {x,y,z} \\\\right)\\\\\\]are not\n",
      "\n",
      "\\right)$$ and $$\\nabla d\\left( {x,y,z} \\right)$$are not\n",
      "\n",
      "manifold curve, and therefore we can describe \\\\\\[\\\\nabla f\\\\left(\n",
      "\n",
      "manifold curve, and therefore we can describe $$\\nabla f\\left(\n",
      "\n",
      "such that \\\\\\[\\\\nabla f\\\\left( {x,y,z} \\\\right) = \\\\lambda \\\\nabla\n",
      "\n",
      "such that $$\\nabla f\\left( {x,y,z} \\right) = \\lambda \\nabla\n",
      "\n",
      "\\\\\\[\\\\begin{array}{l}\\\\mathop {\\\\min }\\\\limits\\_{x,y,z} f\\\\left( {x,y,z}\n",
      "\n",
      "$$\\begin{array}{l}\\mathop {\\min }\\limits\\_{x,y,z} f\\left( {x,y,z}\n",
      "\n",
      "\\\\\\[\\\\begin{array}{l}\\\\mathop {\\\\min }\\\\limits_x f\\\\left( x\n",
      "\n",
      "$$\\begin{array}{l}\\mathop {\\min }\\limits_x f\\left( x\n",
      "\n",
      "1.  The optimal solution \\\\\\[{x\\^\\*}\\\\\\]is inside the constraint region\n",
      "\n",
      "1.  The optimal solution $${x\\^\\*}$$is inside the constraint region\n",
      "\n",
      "    such that\\\\\\[g\\\\left( {{x\\^\\*}} \\\\right) \\< 0\\\\\\]. In this case,\n",
      "\n",
      "    such that$$g\\left( {{x\\^\\*}} \\right) \\< 0$$. In this case,\n",
      "\n",
      "    found by simply solving \\\\\\[\\\\nabla f\\\\left( {{x\\^\\*}} \\\\right) =\n",
      "\n",
      "    found by simply solving $$\\nabla f\\left( {{x\\^\\*}} \\right) =\n",
      "\n",
      "    that\\\\\\[\\\\alpha = 0\\\\\\].\n",
      "\n",
      "    that$$\\alpha = 0$$.\n",
      "\n",
      "2.  The optimal solution\\\\\\[{x\\^\\*}\\\\\\] lies on the constraint boundary\n",
      "\n",
      "2.  The optimal solution$${x\\^\\*}$$ lies on the constraint boundary\n",
      "\n",
      "    such that \\\\\\[g\\\\left( {{x\\^\\*}} \\\\right) = 0\\\\\\]. In this case, we\n",
      "\n",
      "    such that $$g\\left( {{x\\^\\*}} \\right) = 0$$. In this case, we\n",
      "\n",
      "    that\\\\\\[\\\\nabla f\\\\left( {{x\\^\\*}} \\\\right) - \\\\alpha \\\\nabla\n",
      "\n",
      "    that$$\\nabla f\\left( {{x\\^\\*}} \\right) - \\alpha \\nabla\n",
      "\n",
      "    this since we know that \\\\\\[\\\\alpha > 0\\\\\\]. To see why, suppose\n",
      "\n",
      "    this since we know that $$\\alpha > 0$$. To see why, suppose\n",
      "\n",
      "    that \\\\\\[\\\\nabla f\\\\left( {{x\\^\\*}} \\\\right)\\\\\\]was in the same\n",
      "\n",
      "    that $$\\nabla f\\left( {{x\\^\\*}} \\right)$$was in the same\n",
      "\n",
      "    direction as \\\\\\[\\\\nabla g\\\\left( {{x\\^\\*}} \\\\right)\\\\\\], then it is\n",
      "\n",
      "    direction as $$\\nabla g\\left( {{x\\^\\*}} \\right)$$, then it is\n",
      "\n",
      "    that \\\\\\[\\\\nabla f\\\\left( {{x\\^\\*}} \\\\right)\\\\\\]must be in the\n",
      "\n",
      "    that $$\\nabla f\\left( {{x\\^\\*}} \\right)$$must be in the\n",
      "\n",
      "    opposite direction as \\\\\\[\\\\nabla g\\\\left( {{x\\^\\*}} \\\\right)\\\\\\]and\n",
      "\n",
      "    opposite direction as $$\\nabla g\\left( {{x\\^\\*}} \\right)$$and\n",
      "\n",
      "    \\\\\\[\\\\alpha > 0\\\\\\].\n",
      "\n",
      "    $$\\alpha > 0$$.\n",
      "\n",
      "Note that in both cases we have the situation where \\\\\\[\\\\alpha g\\\\left(\n",
      "\n",
      "Note that in both cases we have the situation where $$\\alpha g\\left(\n",
      "\n",
      "1.  Primal feasibility: \\\\\\[g\\\\left( {{x\\^\\*}} \\\\right) \\\\le 0\\\\\\]\n",
      "\n",
      "1.  Primal feasibility: $$g\\left( {{x\\^\\*}} \\right) \\le 0$$\n",
      "\n",
      "2.  Dual feasibility: \\\\\\[\\\\alpha \\\\ge 0\\\\\\]\n",
      "\n",
      "2.  Dual feasibility: $$\\alpha \\ge 0$$\n",
      "\n",
      "3.  Complementary slackness: \\\\\\[\\\\alpha g\\\\left( {{x\\^\\*}} \\\\right) =\n",
      "\n",
      "3.  Complementary slackness: $$\\alpha g\\left( {{x\\^\\*}} \\right) =\n",
      "\n",
      "parameters would be to maximize the log likelihood\\\\\\[L\\\\left( \\\\theta\n",
      "\n",
      "parameters would be to maximize the log likelihood$$L\\left( \\theta\n",
      "\n",
      "distribution\\\\\\[L\\\\left( \\\\theta \\\\right) = \\\\log p\\\\left( {x;\\\\theta }\n",
      "\n",
      "distribution$$L\\left( \\theta \\right) = \\log p\\left( {x;\\theta }\n",
      "\n",
      "\\\\\\]. At this point, we could try to maximize \\\\\\[L\\\\left( \\\\theta\n",
      "\n",
      "$$. At this point, we could try to maximize $$L\\left( \\theta\n",
      "\n",
      "likelihood. The ELBO is a function of the parameters \\\\\\[\\\\theta \\\\\\]\n",
      "\n",
      "likelihood. The ELBO is a function of the parameters $$\\theta $$\n",
      "\n",
      "parameters \\\\\\[\\\\theta \\\\\\]are found by maximizing ELBO over\\\\\\[\\\\theta\n",
      "\n",
      "parameters $$\\theta $$are found by maximizing ELBO over$$\\theta\n",
      "\n",
      "\\\\\\] and Q. It can be shown that for a fixed \\\\\\[\\\\theta \\\\\\], the\n",
      "\n",
      "$$ and Q. It can be shown that for a fixed $$\\theta $$, the\n",
      "\n",
      "latent distribution which maximizes ELBO is \\\\\\[Q\\\\left( z \\\\right) =\n",
      "\n",
      "latent distribution which maximizes ELBO is $$Q\\left( z \\right) =\n",
      "\n",
      "ELBO is to alternate between two steps: 1) fix\\\\\\[\\\\theta \\\\\\]and pick\n",
      "\n",
      "ELBO is to alternate between two steps: 1) fix$$\\theta $$and pick\n",
      "\n",
      "\\\\\\[Q\\\\left( z \\\\right) = p\\\\left( {z\\|x;\\\\theta } \\\\right)\\\\\\], and 2)\n",
      "\n",
      "$$Q\\left( z \\right) = p\\left( {z\\|x;\\theta } \\right)$$, and 2)\n",
      "\n",
      "fix Q and maximize ELBO with respect to \\\\\\[\\\\theta \\\\\\]. This process\n",
      "\n",
      "fix Q and maximize ELBO with respect to $$\\theta $$. This process\n",
      "\n",
      "parameters maximize the joint distribution \\\\\\[P\\\\left( {x,z;\\\\theta }\n",
      "\n",
      "parameters maximize the joint distribution $$P\\left( {x,z;\\theta }\n",
      "\n",
      "    maximize\\\\\\[p\\\\left( {x;\\\\theta } \\\\right)\\\\\\]for a set of\n",
      "\n",
      "    maximize$$p\\left( {x;\\theta } \\right)$$for a set of\n",
      "\n",
      "    variables\\\\\\[p\\\\left( {x;\\\\theta } \\\\right) = \\\\sum\\\\limits_z\n",
      "\n",
      "    variables$$p\\left( {x;\\theta } \\right) = \\sum\\limits_z\n",
      "\n",
      "    \\\\\\[L\\\\left( \\\\theta \\\\right) = \\\\log p\\\\left( {x;\\\\theta } \\\\right)\n",
      "\n",
      "    $$L\\left( \\theta \\right) = \\log p\\left( {x;\\theta } \\right)\n",
      "\n",
      "    be a fixed parameter \\\\\\[\\\\log p\\\\left( {x;\\\\theta } \\\\right) \\\\ge\n",
      "\n",
      "    be a fixed parameter $$\\log p\\left( {x;\\theta } \\right) \\ge\n",
      "\n",
      "    we pick \\\\\\[Q\\\\left( z \\\\right) = p\\\\left( {z\\|x;\\\\theta }\n",
      "\n",
      "    we pick $$Q\\left( z \\right) = p\\left( {z\\|x;\\theta }\n",
      "\n",
      "    \\\\right)\\\\\\]for a specific setting of the parameters\\\\\\[\\\\theta\n",
      "\n",
      "    \\right)$$for a specific setting of the parameters$$\\theta\n",
      "\n",
      "    follows: \\\\\\[ELBO\\\\left( {x;Q,\\\\theta } \\\\right) = \\\\log p\\\\left(\n",
      "\n",
      "    follows: $$ELBO\\left( {x;Q,\\theta } \\right) = \\log p\\left(\n",
      "\n",
      "\\\\\\[E\\\\left\\[ {f\\\\left( X \\\\right)} \\\\right\\] \\\\le f\\\\left( {E\\\\left\\[ X\n",
      "\n",
      "$$E\\left\\[ {f\\left( X \\right)} \\right\\] \\le f\\left( {E\\left\\[ X\n",
      "\n",
      "with equal probability. As shown in the figure, the value \\\\\\[E\\\\left\\[\n",
      "\n",
      "with equal probability. As shown in the figure, the value $$E\\left\\[\n",
      "\n",
      "line segment connecting a and b, while \\\\\\[f\\\\left( {E\\\\left\\[ X\n",
      "\n",
      "line segment connecting a and b, while $$f\\left( {E\\left\\[ X\n",
      "\n",
      "\\\\\\[E\\\\left\\[ {f\\\\left( X \\\\right)} \\\\right\\] \\\\le f\\\\left( {E\\\\left\\[ X\n",
      "\n",
      "$$E\\left\\[ {f\\left( X \\right)} \\right\\] \\le f\\left( {E\\left\\[ X\n",
      "\n",
      "Consider the goal of trying to learn the distribution \\\\\\[p\\\\left(\n",
      "\n",
      "Consider the goal of trying to learn the distribution $$p\\left(\n",
      "\n",
      "\\\\\\[z\\\\\\~N\\\\left( {0,{I\\_{k \\\\times k}}} \\\\right)\\\\\\], and we use\n",
      "\n",
      "$$z\\\\~N\\left( {0,{I\\_{k \\times k}}} \\right)$$, and we use\n",
      "\n",
      "\\\\\\[x\\|z\\\\\\~N\\\\left( {g\\\\left( {z;\\\\theta } \\\\right),{\\\\sigma\n",
      "\n",
      "$$x\\|z\\\\~N\\left( {g\\left( {z;\\theta } \\right),{\\sigma\n",
      "\n",
      "\\^2}{I\\_{d\\\\, \\\\times d}}} \\\\right)\\\\\\]where \\\\\\[g\\\\left( {z;\\\\theta }\n",
      "\n",
      "\\^2}{I\\_{d\\, \\times d}}} \\right)$$where $$g\\left( {z;\\theta }\n",
      "\n",
      "vector and has weights \\\\\\[\\\\theta \\\\\\]. We can compute the likelihood\n",
      "\n",
      "vector and has weights $$\\theta $$. We can compute the likelihood\n",
      "\n",
      "space vector\\\\\\[p\\\\left( {x;\\\\theta } \\\\right) = \\\\sum\\\\limits_z\n",
      "\n",
      "space vector$$p\\left( {x;\\theta } \\right) = \\sum\\limits_z\n",
      "\n",
      "If we could learn the parameters \\\\\\[\\\\theta \\\\\\] which maximize the\n",
      "\n",
      "If we could learn the parameters $$\\theta $$ which maximize the\n",
      "\n",
      "likelihood of the data \\\\\\[p\\\\left( {x;\\\\theta } \\\\right)\\\\\\], then we\n",
      "\n",
      "likelihood of the data $$p\\left( {x;\\theta } \\right)$$, then we\n",
      "\n",
      "by first sampling the latent vector from the Gaussian\\\\\\[z\\\\\\~N\\\\left(\n",
      "\n",
      "by first sampling the latent vector from the Gaussian$$z\\\\~N\\left(\n",
      "\n",
      "from the conditional distribution \\\\\\[x\\|z\\\\\\~N\\\\left( {g\\\\left(\n",
      "\n",
      "from the conditional distribution $$x\\|z\\\\~N\\left( {g\\left(\n",
      "\n",
      "parameters\\\\\\[\\\\theta \\\\\\]. Recall that the ELBO(Q, theta) is a lower\n",
      "\n",
      "parameters$$\\theta $$. Recall that the ELBO(Q, theta) is a lower\n",
      "\n",
      "bound for the log likelihood for all values of Q and\\\\\\[\\\\theta \\\\\\].\n",
      "\n",
      "bound for the log likelihood for all values of Q and$$\\theta $$.\n",
      "\n",
      "can solve for the \\\\\\[\\\\theta \\\\\\] which maximizes the log likelihood by\n",
      "\n",
      "can solve for the $$\\theta $$ which maximizes the log likelihood by\n",
      "\n",
      "\\\\\\[\\\\mathop {\\\\max }\\\\limits_Q \\\\mathop {\\\\max }\\\\limits\\_\\\\theta\n",
      "\n",
      "$$\\mathop {\\max }\\limits_Q \\mathop {\\max }\\limits\\_\\theta\n",
      "\n",
      "the ELBO for a given\\\\\\[\\\\theta \\\\\\]. This was achieved by setting\n",
      "\n",
      "the ELBO for a given$$\\theta $$. This was achieved by setting\n",
      "\n",
      "\\\\\\[Q\\\\left( z \\\\right) = p\\\\left( {z\\|x;\\\\theta } \\\\right)\\\\\\]so that\n",
      "\n",
      "$$Q\\left( z \\right) = p\\left( {z\\|x;\\theta } \\right)$$so that\n",
      "\n",
      "the ELBO touches the log likelihood. Then in the M-step, \\\\\\[\\\\theta\n",
      "\n",
      "the ELBO touches the log likelihood. Then in the M-step, $$\\theta\n",
      "\n",
      "distribution\\\\\\[p\\\\left( {x;\\\\theta } \\\\right)\\\\\\], computing the true\n",
      "\n",
      "distribution$$p\\left( {x;\\theta } \\right)$$, computing the true\n",
      "\n",
      "posterior distribution \\\\\\[p\\\\left( {z\\|x;\\\\theta } \\\\right)\\\\\\]is not\n",
      "\n",
      "posterior distribution $$p\\left( {z\\|x;\\theta } \\right)$$is not\n",
      "\n",
      "tractable since this involves computing \\\\\\[p\\\\left( {x;\\\\theta }\n",
      "\n",
      "tractable since this involves computing $$p\\left( {x;\\theta }\n",
      "\n",
      "rule\\\\\\[p\\\\left( {z\\|x;\\\\theta } \\\\right) = \\\\frac{{p\\\\left(\n",
      "\n",
      "rule$$p\\left( {z\\|x;\\theta } \\right) = \\frac{{p\\left(\n",
      "\n",
      "described as \\\\\\[Q = N\\\\left( {q\\\\left( {x;\\\\phi }\n",
      "\n",
      "described as $$Q = N\\left( {q\\left( {x;\\phi }\n",
      "\n",
      "\\\\\\[\\\\phi \\\\\\]and \\\\\\[\\\\varphi \\\\\\]respectively and are commonly\n",
      "\n",
      "$$\\phi $$and $$\\varphi $$respectively and are commonly\n",
      "\n",
      "referred to as the encoder (likewise \\\\\\[g\\\\left( {z;\\\\theta }\n",
      "\n",
      "referred to as the encoder (likewise $$g\\left( {z;\\theta }\n",
      "\n",
      "expectation over a distribution that depends on \\\\\\[\\\\phi \\\\\\]and\n",
      "\n",
      "expectation over a distribution that depends on $$\\phi $$and\n",
      "\n",
      "\\\\\\[\\\\varphi \\\\\\]. Therefore, evaluating the gradient\n",
      "\n",
      "$$\\varphi $$. Therefore, evaluating the gradient\n",
      "\n",
      "\\\\\\[\\\\begin{array}{l}{\\\\rm{(Line 1) }}{\\\\alpha \\_j}\\\\left( t \\\\right) =\n",
      "\n",
      "$$\\begin{array}{l}{\\rm{(Line 1) }}{\\alpha \\_j}\\left( t \\right) =\n",
      "\n",
      "\\\\\\[attention\\\\left( {q,{\\\\bf{k}},{\\\\bf{v}}} \\\\right) = \\\\sum\\\\limits_i\n",
      "\n",
      "$$attention\\left( {q,{\\bf{k}},{\\bf{v}}} \\right) = \\sum\\limits_i\n",
      "\n",
      "plane that are each a distance of \\\\\\[\\\\frac{1}{{\\\\left\\\\\\| w\n",
      "\n",
      "plane that are each a distance of $$\\frac{1}{{\\left\\\\| w\n",
      "\n",
      "margin size of \\\\\\[\\\\frac{2}{{\\\\left\\\\\\| w \\\\right\\\\\\|}}\\\\\\]. In this\n",
      "\n",
      "margin size of $$\\frac{2}{{\\left\\\\| w \\right\\\\|}}$$. In this\n",
      "\n",
      "follows: maximize the margin distance \\\\\\[\\\\frac{2}{{\\\\left\\\\\\| w\n",
      "\n",
      "follows: maximize the margin distance $$\\frac{2}{{\\left\\\\| w\n",
      "\n",
      "\\\\right\\\\\\|}}\\\\\\] (equivalent to minimizing \\\\\\[\\\\left\\\\\\| w\n",
      "\n",
      "\\right\\\\|}}$$ (equivalent to minimizing $$\\left\\\\| w\n",
      "\n",
      "\\\\\\[\\\\frac{1}{{\\\\left\\\\\\| w \\\\right\\\\\\|}}\\\\\\]from the decision boundary\n",
      "\n",
      "$$\\frac{1}{{\\left\\\\| w \\right\\\\|}}$$from the decision boundary\n",
      "\n",
      "being \\\\\\[\\\\frac{1}{{\\\\left\\\\\\| w \\\\right\\\\\\|}}\\\\\\]is encoded in the\n",
      "\n",
      "being $$\\frac{1}{{\\left\\\\| w \\right\\\\|}}$$is encoded in the\n",
      "\n",
      "inequality since \\\\\\[{w\\^T}\\\\left( {\\\\frac{w}{{{{\\\\left\\\\\\| w\n",
      "\n",
      "inequality since $${w\\^T}\\left( {\\frac{w}{{{{\\left\\\\| w\n",
      "\n",
      "\\\\\\[\\\\left\\\\\\| {\\\\frac{w}{{{{\\\\left\\\\\\| w \\\\right\\\\\\|}\\^2}}}}\n",
      "\n",
      "$$\\left\\\\| {\\frac{w}{{{{\\left\\\\| w \\right\\\\|}\\^2}}}}\n",
      "\n",
      "\\\\\\[{y_n}{w\\^T}\\\\phi \\\\left( {{x_n}} \\\\right) = 1\\\\\\]when the data point\n",
      "\n",
      "$${y_n}{w\\^T}\\phi \\left( {{x_n}} \\right) = 1$$when the data point\n",
      "\n",
      "is a distance of \\\\\\[\\\\frac{1}{{\\\\left\\\\\\| w \\\\right\\\\\\|}}\\\\\\]from the\n",
      "\n",
      "is a distance of $$\\frac{1}{{\\left\\\\| w \\right\\\\|}}$$from the\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('formatted_output.md', 'w') as output_file:\n",
    "    with open('output.md') as in_file:\n",
    "        for i,line in enumerate(in_file):\n",
    "\n",
    "            \n",
    "            if '\\[' in line:\n",
    "                print(line)\n",
    "                \n",
    "                line = line.replace('\\\\\\\\\\\\[', '$$')\n",
    "                line = line.replace('\\\\\\\\\\\\]', '$$')\n",
    "                line = line.replace('\\\\\\\\', '\\\\')\n",
    "                print(line)\n",
    "                \n",
    "            output_file.write(line)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15e1e109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\\\]\n"
     ]
    }
   ],
   "source": [
    "print('\\\\\\]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b299814",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
